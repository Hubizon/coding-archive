{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning to dziedzina mocno eksperymentalna, a co za tym idzie, dla danego problemu może istnieć wiele zadowalających rozwiązań (zazwyczaj nieoptymalnych). Często zdarza się, że architektury sieci neuronowych są nieproporcjonalnie duże do stopnia skomplikowania zadania. Okazuje się, że możemy odchudzić modele z tylko niewielką stratą poprawności predykcji.\n",
    "\n",
    "**Pruning** (przycinanie) sieci polega na usunięciu poszczególnych wag lub całych neuronów. Istnieje wiele zalet tej metody:\n",
    "- zmniejszenie rozmiaru sieci,\n",
    "- przyspieszenie inferencji,\n",
    "- przeciwdziałanie przeuczeniu,\n",
    "- polepszenie wyników.\n",
    "\n",
    "Aby skutecznie zmniejszyć rozmiar sieci, musimy wyzerować odpowiednio dużo elementów w jej macierzach wag. Dzięki temu, będziemy w stanie lepiej skompresować model w pamięci. Jednak samo wyzerowanie wag nie wystarczy do przyspieszenia inferencji. Należy dodatkowo zaimplementować i skutecznie wykorzystać liczenie macierzy rzadkich (Sparse Matrix). Inną metodą przycinania może być usuwanie całych neuronów - zmniejszając w ten sposób faktyczny rozmiar macierzy wag.\n",
    "\n",
    "W tym zadaniu skupimy się **tylko** na **zerowaniu wag** w modelu. **Nie można zmieniać architektury sieci** (np. poprzez usunięcie neuronu lub całej ukrytej warstwy). Rozpatrzymy ten problem na przykładzie **regresji**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie\n",
    "\n",
    "Zaimplementuj funkcję `your_pruning_algorithm(model : torch.nn.Module) -> pruned_model: torch.nn.Module`, która przyjmie na wejściu zaimplementowany poniżej model i zwróci jego wyczyszczoną wersję - tzn. z jak największą liczbą wyzerowanych parametrów modelu (wag i stałych [weights and biases]), przy zachowaniu jak najniższego średniego błędu kwadratowego (MSE) predykcji.\n",
    "\n",
    "Poniżej w notatniku znajdziesz komórkę, w której znajduje się miejsce na Twoją funkcję. Komórki, które będziesz miał zmodyfikować będą bardzo jasno oznaczone!\n",
    "\n",
    "Oceniany będziesz na podstawie wyniku poniższej funkcji (im wyższa wartość tym lepiej):\n",
    "\n",
    "$$\n",
    "\\mathrm{score}(s, \\epsilon) = \\begin{cases}\n",
    "    0 & \\text{jeżeli } \\epsilon > 1000 \\\\\n",
    "    (1 - \\frac{\\epsilon}{1000})^{1.5} \\cdot s ^{1.5} & \\text{w.p.p.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "- $s$ - liczba zerowych parametrów modelu podzielona przez liczbę wszystkich parametrów modelu (sparsity)\n",
    "- $\\epsilon$ - średni błąd kwadratowy na zbiorze testowym (MSE)\n",
    "\n",
    "To kryterium i wszystkie funkcje, o których mowa powyżej, są zaimplementowane poniżej przez nas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ograniczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twoja funkcja powinna zwracać model w maksymalnie 5 minut używając Google Colab z GPU.\n",
    "- Plik z wagami powinien być zapisany funkcją `save_parameters` pod nazwą `model_parameters.pkl`.\n",
    "\n",
    "- **Nie możesz** zmieniać architektury modelu, tzn. musi on mieć dokładnie:\n",
    "    - warstwę wejściową o rozmiarze 128\n",
    "    - warstwę ukrytą o rozmiarze 1024\n",
    "    - funkcję aktywacji Sigmoid\n",
    "    - warstwę wyjściową o rozmiarze 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pliki zgłoszeniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ten notebook\n",
    "* Parametry (wagi) modelu, zapisane funkcją `save_parameters`. **Nie zmieniaj** nazwy wygenerowanego pliku: `model_parameters.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oceniony zostanie dostarczony przez Ciebie plik z wagami. Powinieneś dostarczyć jednak również działający notebook, który po uruchomieniu wszystkich komórek z flagą `FINAL_EVALUATION_MODE` ustawioną na `True` wyprodukuje plik z wagami `model_parameters.pkl` w czasie poniżej 5 minut (liczonym na Google Colab z dostępem do GPU). \n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 1.5 punkta. Jeśli dostaniesz `score` poniżej 0.085, to dostaniesz 0, a jeśli powyżej 0.95 to dostaniesz 1.5 punkt. Pomiędzy tymi wartościami, twój wynik rośnie liniowo z wartością `score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOxhj9Y-I3jp"
   },
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_EVALUATION_MODE = False  # Podczas sprawdzania ustawimy tą flagę na True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGPyTemjs4Hf"
   },
   "source": [
    "## Ładowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CGT6a6HNLUsb"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Funkcja ładująca dane treningowe i walidacyjne jako np.array\n",
    "def load_data_from_file(x_train_path, y_train_path, x_valid_path, y_valid_path):\n",
    "    X_train = np.load(x_train_path)\n",
    "    y_train = np.load(y_train_path)\n",
    "\n",
    "    X_valid = np.load(x_valid_path)\n",
    "    y_valid = np.load(y_valid_path)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# Klasa zbioru dancyh\n",
    "class InMemDataset(Dataset):\n",
    "    def __init__(self, xs, ys, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.dataset = []\n",
    "        for i in tqdm(range(len(xs))):\n",
    "            self.dataset.append((torch.tensor(xs[i]).to(device).float(), torch.tensor(ys[i]).to(device).float() ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "ab5451d0e36d42b8a9624e9ec4ad89d2",
      "cbdac4f64f3f4829b0d2f20defbaf6fc",
      "35db74dcedc843d58c278e88478cfe4d",
      "c6b58dbb52914e4ea69842463c9c26bd",
      "a9d7aba8328d47db9cd2e0f3baf46756",
      "384871d0b7184754b8009864b20b698c",
      "da4f83f72f5e488696fc6339fc0884bd",
      "5da6fd73e9b444fa88ae2947b649892d",
      "d43483dbdc8442fb98c45f81ebb19e67",
      "0ed577d44c5f41d1a5b076230d683d4e",
      "6ad046a50baa4e8b89d453d679f1ad2a",
      "c9d46f9e190b4bbf98eaaa48a6194baf",
      "51985fd41ab7488ea67a9ae80f038814",
      "2ebb14bb332e4ece96ecaf28d92d3881",
      "546157c892fc43fbafd3510061980ed5",
      "e4be3e6d27bd4d1d88cefb711fabb985",
      "b420701e539a4751bd7a5b3f7a89578e",
      "d6ac598121c54b668825d9eef1a35727",
      "9f76d4fa1b514146a7835caad99e7fdf",
      "742ec004672d4e99b761dddf25345e69",
      "8ba08fe611444330a1e7c3e53a757524",
      "e5138d6ec12442f98a68ddc747c06778"
     ]
    },
    "id": "6wUnuyGDJ7Lv",
    "outputId": "6248dc89-67c2-44b7-f4fd-7cf875539c2e"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Wczytajmy dane i stwórzmy dataloadery\n",
    "X_train, y_train, X_valid, y_valid = load_data_from_file(\n",
    "    \"train_data/X_train.npy\",\n",
    "    \"train_data/y_train.npy\",\n",
    "    \"valid_data/X_valid.npy\",\n",
    "    \"valid_data/y_valid.npy\",\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "_train = InMemDataset(X_train, y_train, device)\n",
    "\n",
    "_valid = InMemDataset(X_valid, y_valid, device)\n",
    "\n",
    "loaders = {\n",
    "    \"train\" : DataLoader(_train, batch_size=batch_size, shuffle=True),\n",
    "    \"valid\" : DataLoader(_valid, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kod z kryterium oceniającym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Całkowite kryterium zdefiniowane w treści zadania\n",
    "def score(mse_loss, sparsity, mse_weight=1.5, sparsity_weight=1.5):\n",
    "\n",
    "    if type(mse_loss) == np.ndarray:\n",
    "        mse_loss[mse_loss > 1000] = 1000\n",
    "    else:\n",
    "        if mse_loss > 1000:\n",
    "            mse_loss = 1000\n",
    "\n",
    "    score = (1 - mse_loss / 1000) ** mse_weight * sparsity**sparsity_weight\n",
    "    return score\n",
    "\n",
    "# Stosunek wyzerowanych wag do wszystkich\n",
    "def get_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "# Błąd średniokwadratowy (MSE)\n",
    "def compute_error(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = 0\n",
    "    num_of_el = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            num_of_el += x.shape[0] * y.shape[1]\n",
    "            losses += model.loss(outputs, y, reduction=\"sum\")\n",
    "\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "def points(score):\n",
    "    def scale(x, lower=0.085, upper=0.95, max_points=1.5):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower) * max_points\n",
    "    return scale(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2xI0m8vSDPD"
   },
   "source": [
    "## Architektura Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ejmg01YQSDkP"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Zdefiniujmy architekturę naszej sieci\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, input, target, reduction=\"mean\"):\n",
    "        mse_loss = nn.MSELoss(reduction=reduction)\n",
    "        return mse_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DKHmJ6Vx_yvS"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Inicializacja wag sieci\n",
    "def init_weights(m):\n",
    "    ''' Initialize the weights in the module m.'''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Funkcja do zapisywania wag modelu do pliku - pamiętaj, aby Twój plik z wagami nazywał się: model_parameters.pkl\n",
    "def save_parameters(model, file_name=\"model_parameters.pkl\", to_file=True):\n",
    "\n",
    "    params_to_save = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params_to_save[name] = param.to(\"cpu\")\n",
    "    \n",
    "    if not to_file:\n",
    "        return params_to_save\n",
    "    \n",
    "    with open(f\"{file_name}\", \"wb\") as f:\n",
    "        pickle.dump(params_to_save, f)\n",
    "\n",
    "\n",
    "# Funkcja do wczytywania wag modelu z pliku\n",
    "def load_parameters(model, file_name=\"model_parameters.pkl\", from_file=True, params=None):\n",
    "\n",
    "    if from_file:\n",
    "        with open(f\"{file_name}\", \"rb\") as f:\n",
    "            params_to_load = pickle.load(f)\n",
    "    else:\n",
    "        params_to_load = params\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            param[...] = params_to_load[name].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxDq_UaTSIqE"
   },
   "source": [
    "## Trening Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jJDPgiFDA52R"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Funkcja do trenowania modelu\n",
    "def train_model(model: nn.Module,\n",
    "              data_loaders: dict[str, DataLoader],\n",
    "              num_epochs: int,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              verbose: bool =True\n",
    "              ) -> tuple[torch.Tensor, float]:\n",
    "\n",
    "    \"\"\"Funkcja do trenowania modelu.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Sieć neuronowa do wytrenowania.\n",
    "        data_loaders (dict[str, DataLoader]): Dictionary zawierający DataLoadery dla zbiorów treninowego i walidacyjnego.\n",
    "        num_epochs (int): Liczba epok do treningu.\n",
    "        optimizer (torch.optim.Optimizer): Optymalizator do trenowania modelu.\n",
    "        verbose (bool, optional): Jeżeli True, pokazuje postęp treningu.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, float]: Tuple zawierający najlepszy zestaw parametrów modelu znalezionych podczas treningu oraz odpowiadającą wartość funkcji straty na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    if FINAL_EVALUATION_MODE:\n",
    "        verbose = False\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in data_loaders['train']:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "\n",
    "            _iter +=1\n",
    "\n",
    "        val_loss = compute_error(model, data_loaders[\"valid\"])\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    return best_params, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "id3_39t7CoQH",
    "outputId": "42f277c8-7710-4fca-c610-cb6bb5f96631"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "initial_model = MLP().to(device)\n",
    "initial_model.apply(init_weights)\n",
    "\n",
    "optimizer = SGD(\n",
    "    initial_model.parameters(),\n",
    "    lr = 0.01,\n",
    "    momentum = 0.95,\n",
    "    weight_decay = 0.001)\n",
    "\n",
    "best_params, best_val_loss = train_model(initial_model, loaders, num_epochs=100, optimizer=optimizer, verbose=True)\n",
    "\n",
    "loss = compute_error(initial_model, loaders[\"valid\"])\n",
    "m = f\"| Validation loss: {loss:>5.2f} |\"\n",
    "print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przykładowe rozwiązanie\n",
    "Poniżej prezentujemy proste rozwiązanie, które w oczywisty sposób nie jest optymalne. Służy tylko temu, aby było wiadomo, w jaki sposób ma działać cały notatnik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starter_pruning_algorithm(model):\n",
    "    with torch.no_grad():\n",
    "        model.layers[0].weight[:, 0:2] = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    # Zróbmy głęboką kopię, aby nie zmieniać wag wytrenowanego modelu\n",
    "    model_to_prune = copy.deepcopy(initial_model)\n",
    "\n",
    "    # Przytnijmy wagi przykładowym rozwiązaniem\n",
    "    model_to_prune = starter_pruning_algorithm(model_to_prune)\n",
    "\n",
    "    # Zapisywanie parametrów modelu (tutaj zmieniliśmy nazwę pliku, Ty zapisuj jako \"model_parameters.pkl\")\n",
    "    save_parameters(model_to_prune, \"starter_model_parameters.pkl\")\n",
    "\n",
    "    # Zobaczmy teraz jak wczytać wcześniej zapisane wagi do nowo utworzonego modelu\n",
    "    new_model = MLP().to(device)\n",
    "    loss = compute_error(new_model, loaders[\"valid\"])\n",
    "    print(f\"Nowy model ma loss {loss:.3f}\")\n",
    "\n",
    "    # Wczytywanie parametrów modelu\n",
    "    load_parameters(new_model, \"starter_model_parameters.pkl\")\n",
    "    loss = compute_error(new_model, loaders[\"valid\"])\n",
    "    print(f\"Po wczytaniu parametrów model ma loss {loss:.3f}\")\n",
    "\n",
    "    mse = compute_error(new_model, loaders[\"valid\"])\n",
    "    sparsity = get_sparsity(new_model)\n",
    "\n",
    "    print(f\"MSE modelu: {mse:.3f} Sparsity: {sparsity:.3f}\")\n",
    "    model_score = score(mse, sparsity)\n",
    "    print(f\"Wynik twojego modelu to {model_score:.3f}!\")\n",
    "    print(f\"Twoje rozwiązanie zdobywa {points(model_score):.3f}/1.5 punktów!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqNhSEw5Z0Ak"
   },
   "source": [
    "# Twoje rozwiązanie\n",
    "\n",
    "Ta sekcja to jedyne miejsce, gdzie możesz zmieniać kod!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubert Jastrzębski - V LO Kraków\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_model(model: nn.Module, \n",
    "              data_loaders: dict[str, DataLoader],\n",
    "              num_epochs: int,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              masks: list[torch.Tensor],\n",
    "              verbose: bool = False\n",
    "              ) -> tuple[torch.Tensor, float]:\n",
    "    if FINAL_EVALUATION_MODE:\n",
    "        verbose = False\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in data_loaders['train']:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "\n",
    "            _iter +=1\n",
    "            \n",
    "            masks_i = 0\n",
    "            with torch.no_grad():\n",
    "                for param in [model.layers[i].weight for i in [0, 2]]:\n",
    "                    param.data.mul_(masks[masks_i])\n",
    "                    masks_i += 1\n",
    "\n",
    "        val_loss = compute_error(model, data_loaders[\"valid\"])\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    return best_params, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXc25J9-_r8J"
   },
   "outputs": [],
   "source": [
    "def your_pruning_algorithm(model):\n",
    "    pruned_model = starter_pruning_algorithm(model)\n",
    "\n",
    "    best_model = pruned_model\n",
    "    best_res = 0\n",
    "\n",
    "    for percent, wd, ep in [(.1, .2, 30), (.3, .15, 30), (.5, .12, 30), (.7, .1, 30), (.8, .08, 30), \n",
    "                            (.9, .03, 30), (.925, .01, 30), (.95, .005, 100),\n",
    "                           (.96, .001, 50), (.967, .0007, 150), (.97, .0004, 100), (.971, .0002, 150),\n",
    "                           (.972, .00007, 150), (.973, .00003, 150), (.9735, .00002, 150), (.974, 0, 150)]:\n",
    "\n",
    "        masks = []\n",
    "        with torch.no_grad():\n",
    "            all_weights = []\n",
    "            for param in pruned_model.parameters():\n",
    "                all_weights.extend(param.view(-1).cpu().detach().numpy())\n",
    "\n",
    "            threshold_index = int(percent * len(all_weights))\n",
    "            threshold_value = np.sort(np.abs(all_weights))[threshold_index]\n",
    "\n",
    "            for param in [pruned_model.layers[i].weight for i in [0, 2]]:\n",
    "                mask = torch.abs(param) > threshold_value\n",
    "                param.data.mul_(mask)\n",
    "                masks.append(mask)\n",
    "        \n",
    "\n",
    "        my_optimizer = SGD(pruned_model.parameters(), lr=0.006, momentum=0.95, weight_decay=wd)\n",
    "        best_params, best_val_loss = my_train_model(pruned_model, loaders, num_epochs=ep, optimizer=my_optimizer, masks=masks, verbose=False)\n",
    "\n",
    "        mse = compute_error(pruned_model, loaders[\"valid\"])\n",
    "        sparsity = get_sparsity(pruned_model)\n",
    "\n",
    "        res = score(mse, sparsity)\n",
    "        if (res > best_res):\n",
    "            best_model = copy.deepcopy(pruned_model)\n",
    "            best_res = res\n",
    "\n",
    "    save_parameters(best_model, \"model_parameters.pkl\")\n",
    "    return best_model\n",
    "\n",
    "model_to_prune = copy.deepcopy(initial_model)\n",
    "your_pruning_algorithm(model_to_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod będzie służył ewaluacji rozwiązania. Po wysłaniu rozwiązania do nas, zostanie wykonana funkcja `evaluate_algorithm(X_valid, y_valid)`, t.j. prawie identyczny kod jak niżej będzie się uruchamiał na katalogu zdjęć `test_data` dostępnym tylko dla sprawdzających zadania.\n",
    "\n",
    "Upewnij się przed wysłaniem, że cały notebook (również z ustawioną flagą `FINAL_EVALUATION_MODE = True`) wykonuje się od początku do końca bez błędów, bez ingerencji użytkownika i zapisuje wagi w pliku `model_parameters.pkl` po wykonaniu polecenia `Run All`. Sprawdź też, czy `validation_script.py` zwraca spodziewany wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test):\n",
    "    \"\"\"Validator\"\"\"\n",
    "    test_model = MLP().to(device)\n",
    "    load_parameters(test_model)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    _test = InMemDataset(X_test, y_test, device)\n",
    "    test_loader = DataLoader(_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    mse = compute_error(test_model, test_loader)\n",
    "    sparsity = get_sparsity(test_model)\n",
    "\n",
    "    print(f\"Model had error: {mse:.3f} and sparsity: {sparsity:.3f}\")\n",
    "    model_score = score(mse, sparsity)\n",
    "\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:    \n",
    "    model_score = evaluate(X_valid, y_valid)\n",
    "    print(f\"Your solution gets score {model_score:.3f} on validation set.\")\n",
    "    print(f\"Your solution gets {points(model_score):.3f}/1.5 points on validation set!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6dGngkAS3kL2",
    "XOxhj9Y-I3jp",
    "Ve2xsZ6gJBwi",
    "EGPyTemjs4Hf",
    "W2xI0m8vSDPD",
    "VYGrm2apHEMn",
    "sxDq_UaTSIqE",
    "zqNhSEw5Z0Ak"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed577d44c5f41d1a5b076230d683d4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ebb14bb332e4ece96ecaf28d92d3881": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f76d4fa1b514146a7835caad99e7fdf",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_742ec004672d4e99b761dddf25345e69",
      "value": 2000
     }
    },
    "35db74dcedc843d58c278e88478cfe4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5da6fd73e9b444fa88ae2947b649892d",
      "max": 8000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d43483dbdc8442fb98c45f81ebb19e67",
      "value": 8000
     }
    },
    "384871d0b7184754b8009864b20b698c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51985fd41ab7488ea67a9ae80f038814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b420701e539a4751bd7a5b3f7a89578e",
      "placeholder": "​",
      "style": "IPY_MODEL_d6ac598121c54b668825d9eef1a35727",
      "value": "100%"
     }
    },
    "546157c892fc43fbafd3510061980ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ba08fe611444330a1e7c3e53a757524",
      "placeholder": "​",
      "style": "IPY_MODEL_e5138d6ec12442f98a68ddc747c06778",
      "value": " 2000/2000 [00:00&lt;00:00, 16436.84it/s]"
     }
    },
    "5da6fd73e9b444fa88ae2947b649892d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ad046a50baa4e8b89d453d679f1ad2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "742ec004672d4e99b761dddf25345e69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ba08fe611444330a1e7c3e53a757524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f76d4fa1b514146a7835caad99e7fdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9d7aba8328d47db9cd2e0f3baf46756": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab5451d0e36d42b8a9624e9ec4ad89d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbdac4f64f3f4829b0d2f20defbaf6fc",
       "IPY_MODEL_35db74dcedc843d58c278e88478cfe4d",
       "IPY_MODEL_c6b58dbb52914e4ea69842463c9c26bd"
      ],
      "layout": "IPY_MODEL_a9d7aba8328d47db9cd2e0f3baf46756"
     }
    },
    "b420701e539a4751bd7a5b3f7a89578e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b58dbb52914e4ea69842463c9c26bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ed577d44c5f41d1a5b076230d683d4e",
      "placeholder": "​",
      "style": "IPY_MODEL_6ad046a50baa4e8b89d453d679f1ad2a",
      "value": " 8000/8000 [00:00&lt;00:00, 23585.93it/s]"
     }
    },
    "c9d46f9e190b4bbf98eaaa48a6194baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51985fd41ab7488ea67a9ae80f038814",
       "IPY_MODEL_2ebb14bb332e4ece96ecaf28d92d3881",
       "IPY_MODEL_546157c892fc43fbafd3510061980ed5"
      ],
      "layout": "IPY_MODEL_e4be3e6d27bd4d1d88cefb711fabb985"
     }
    },
    "cbdac4f64f3f4829b0d2f20defbaf6fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_384871d0b7184754b8009864b20b698c",
      "placeholder": "​",
      "style": "IPY_MODEL_da4f83f72f5e488696fc6339fc0884bd",
      "value": "100%"
     }
    },
    "d43483dbdc8442fb98c45f81ebb19e67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6ac598121c54b668825d9eef1a35727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da4f83f72f5e488696fc6339fc0884bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4be3e6d27bd4d1d88cefb711fabb985": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5138d6ec12442f98a68ddc747c06778": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
