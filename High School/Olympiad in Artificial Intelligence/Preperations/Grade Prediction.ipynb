{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.education = [35, 36, 37, 26, 11, 30, 29, 14, 10, 12, 18, \n",
    "            13, 27, 19, 9, 1, 25, 20, 22, 31, 33, 6, 2, 40, \n",
    "            3, 4, 43, 41, 42, 39, 5, 44, 34]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        for i in range(len(X)):\n",
    "            try:\n",
    "                X[i, 0] = self.education.index(X[i, 0])\n",
    "            except ValueError:\n",
    "                X[i, 0] = len(self.education)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features\n",
    "\n",
    "class OccPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.occupation = [99, 0, 90, 195, 9, 192, 193, 191, 194, 6, 161, \n",
    "            163, 7, 171, 172, 173, 174, 175, 8, 181, 182, 183, 5, 151, 152, \n",
    "            153, 154, 4, 141, 143, 144, 3, 131, 132, 134, 135, 2, 121, 122, \n",
    "            123, 124, 125, 1, 112, 114, 10, 101, 102, 103]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        for i in range(len(X)):\n",
    "            try:\n",
    "                X[i, 0] = self.occupation.index(X[i, 0])\n",
    "            except ValueError:\n",
    "                X[i, 0] = len(self.occupation)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('onehot_encoder', OneHotEncoder(min_frequency=0.01))\n",
    "    ])\n",
    "\n",
    "ed_pipeline = Pipeline([\n",
    "    ('ed_custom', EdPipeline()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "occ_pipeline = Pipeline([\n",
    "    ('occ_custom', OccPipeline()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "num_attribs = ['Application order', 'Previous qualification (grade)', 'Admission grade', \n",
    "    'Age at enrollment', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate', 'Inflation rate', 'GDP']\n",
    "cat_attribs = ['Marital status', 'Application mode', 'Course', 'Daytime/evening attendance',\n",
    "    'Nacionality', 'Displaced', 'Educational special needs', 'Debtor', \n",
    "    'Tuition fees up to date', 'Gender', 'Scholarship holder', 'International']\n",
    "ed_attribs = ['Previous qualification', \"Mother's qualification\", \"Father's qualification\"]\n",
    "occ_attribs = [\"Mother's occupation\", \"Father's occupation\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', cat_pipeline, cat_attribs),\n",
    "    ('ed', ed_pipeline, ed_attribs),\n",
    "    ('occ', occ_pipeline, occ_attribs)\n",
    "])\n",
    "\n",
    "data_prepared = full_pipeline.fit_transform(dataset)\n",
    "feauture_names = full_pipeline.get_feature_names_out()\n",
    "dataset_prepared = pd.DataFrame(data_prepared, columns=feauture_names)\n",
    "dataset_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset['Target'].to_numpy().reshape(-1, 1)\n",
    "ord_encoder = OrdinalEncoder()\n",
    "targets = ord_encoder.fit_transform(targets)\n",
    "in_features = data_prepared.shape[1]\n",
    "data_prepared.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_prepared_t = torch.tensor(data_prepared, dtype=torch.float, device=device)\n",
    "targets_t = torch.tensor(targets.flatten(), dtype=torch.long, device=device)\n",
    "tensor_dataset = TensorDataset(data_prepared_t, targets_t)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(tensor_dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 3),\n",
    "        )\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.layers[0].weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layers[3].weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layers[6].weight, nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to train a model\n",
    "\n",
    "def compute_error(model, data_loader, criterion, c_sum=False):\n",
    "    model.eval()\n",
    "    losses, num_of_el = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if not c_sum: loss *= len(y)\n",
    "            losses += loss\n",
    "            num_of_el += len(y)\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "              train_loader: DataLoader,\n",
    "              valid_loader: DataLoader,\n",
    "              num_epochs: int,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion,\n",
    "              verbose: bool = True,\n",
    "              verbose_plot: bool = False\n",
    "              ) -> float:\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "            _iter += 1\n",
    "\n",
    "        val_loss = compute_error(model, valid_loader, criterion)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "        if verbose_plot:\n",
    "            train_loss = compute_error(model, train_loader, criterion)\n",
    "            train_losses.append(train_loss.detach().cpu())\n",
    "            valid_losses.append(val_loss.detach().cpu())\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    if verbose_plot:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(train_losses, c='b', label='train')\n",
    "        plt.plot(valid_losses, c='r', label='valid')\n",
    "        plt.grid(ls=':')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, valid_loader, 30, optimizer, criterion, verbose_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy on a single and multiple models\n",
    "\n",
    "def accuracy(outputs, y):\n",
    "    pred = outputs.argmax(dim=1)\n",
    "    return sum(pred == y)\n",
    "\n",
    "print(compute_error(model, valid_loader, accuracy, c_sum=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into k folds and train k models on them\n",
    "\n",
    "def train_kfold(Net, dataset, n_splits=5, num_epochs=10, batch_size=32, learning_rate=0.03):\n",
    "    models = []\n",
    "    scores = []\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    for train_ids, val_ids in kf.split(dataset):\n",
    "        train_sub = Subset(dataset, train_ids)\n",
    "        valid_sub = Subset(dataset, val_ids)\n",
    "        train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = Net().to(device)\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_model(model, train_loader, valid_loader, num_epochs=num_epochs, \n",
    "                    optimizer=optimizer, criterion=criterion, verbose=True)\n",
    "\n",
    "        scores.append(compute_error(model, valid_loader, criterion).detach().cpu())\n",
    "        models.append((model, 0))\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "\n",
    "# training the models and checking the scores\n",
    "models, scores = train_kfold(Net, train_dataset, n_splits=10, num_epochs=30, batch_size=64, learning_rate=0.001)\n",
    "clear_output(False)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "for x_batch, y_batch in train_loader:\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        X_train.append(x.detach().cpu().numpy())\n",
    "        y_train.append(y.detach().cpu().numpy())\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_valid, y_valid = [], []\n",
    "for x_batch, y_batch in valid_loader:\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        X_valid.append(x.detach().cpu().numpy())\n",
    "        y_valid.append(y.detach().cpu().numpy())\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = ensemble.RandomForestClassifier(n_estimators=150, max_depth=60, criterion='log_loss', n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = rf_clf.predict(X_valid)\n",
    "sum(pred == y_valid) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = ensemble.GradientBoostingClassifier()\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = gb_clf.predict(X_valid)\n",
    "sum(pred == y_valid) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    rf_clf = ensemble.RandomForestClassifier(n_estimators=150, max_depth=60, criterion='log_loss', n_jobs=-1)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    models.append((rf_clf, 1))\n",
    "    \n",
    "    gb_clf = ensemble.GradientBoostingClassifier()\n",
    "    gb_clf.fit(X_train, y_train)    \n",
    "    models.append((gb_clf, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to predict outputs with k models\n",
    "\n",
    "def predict_ensemble(models, x):\n",
    "    predictions = []\n",
    "    for model, t in models:\n",
    "        if t == 0:\n",
    "            model.eval()\n",
    "            model_preds = []\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                model_preds.append(pred)\n",
    "            predictions.append(torch.cat(model_preds))\n",
    "        else:\n",
    "            pred = model.predict(x.detach().cpu())\n",
    "            t_pred = torch.zeros((x.shape[0], 3), device=device)\n",
    "            t_pred[np.arange(x.shape[0]), pred] = 1\n",
    "            predictions.append(t_pred)\n",
    "\n",
    "    predictions = torch.mean(torch.stack(predictions), dim=0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, data_loader, criterion, c_sum=False):\n",
    "    losses, num_of_el = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = predict_ensemble(models, x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if not c_sum: loss *= len(y)\n",
    "            losses += loss\n",
    "            num_of_el += len(y)\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "print(evaluate_ensemble(models, valid_loader, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy on a single and multiple models\n",
    "\n",
    "def accuracy(outputs, y):\n",
    "    pred = outputs.argmax(dim=1)\n",
    "    return sum(pred == y)\n",
    "\n",
    "print(compute_error(models[0][0], valid_loader, accuracy, c_sum=True))\n",
    "print(evaluate_ensemble(models, valid_loader, accuracy, c_sum=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(\"test.csv\")\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = full_pipeline.fit_transform(dataset_test)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_t = torch.tensor(data_test, device=device, dtype=torch.float)\n",
    "logits_test = predict_ensemble(models, data_test_t)\n",
    "\n",
    "pred_test = logits_test.argmax(dim=1)\n",
    "pred_test_c = np.array(ord_encoder.categories_).flatten()[np.array(pred_test.detach().cpu())]\n",
    "pred_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_dataset = pd.DataFrame({ 'id': dataset_test['id'].values,'Target': pred_test_c })\n",
    "submit_dataset.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
