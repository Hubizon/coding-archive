{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O7aK_zHPEu8"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezT5FVxBARd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal, Uniform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFXoowjW9l--"
   },
   "source": [
    "# Introduction to Gaussian distribution\n",
    "\n",
    "PDF of the Gaussian distribution is defined as:\n",
    "$$ p(x) = N(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n",
    " \\qquad x \\in \\mathbb{R}.$$\n",
    " where mean $\\mu$ tells us what is the \"center\" of the distribution and variance $\\sigma^2$ how the distribution is spread from the mean.\n",
    "\n",
    "The cool thing about normal distribution is that we can easily change the mean and variance of a random variable. Let's say we have $X \\sim N(0, 1)$ and we want to change it to $Y \\sim N(\\mu, \\sigma^2)$. We can do it as follows:\n",
    "$$ Y = X * \\sigma + \\mu $$\n",
    "Note that we're multiplying by standard deviation $\\sigma$, not the variance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxgcsAMsANjH"
   },
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return 1 / np.sqrt(2 * np.pi * sigma ** 2) * np.exp(- (x - mu) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def gaussian_sample(n, mu, sigma):\n",
    "    samples_standard = np.random.randn(n)\n",
    "    return sigma * samples_standard + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1718307310030,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "HY2uRA6MC1V9",
    "outputId": "a9ddc0d8-fb86-440e-d5cb-52f8d901e14c"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-4,4,1000)\n",
    "pdf_standard = gaussian_pdf(x, 0, 1)\n",
    "pdf_shifted = gaussian_pdf(x, -1, 1)\n",
    "pdf_scaled = gaussian_pdf(x, 0, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, pdf_standard, label='0, 1')\n",
    "plt.plot(x, pdf_shifted, label='-1, 1')\n",
    "plt.plot(x, pdf_scaled, label='0, 4')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1718307561641,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "QrZu0_qDGEcD",
    "outputId": "c619434d-0b82-4112-b31d-c9ce4c4d291b"
   },
   "outputs": [],
   "source": [
    "mu = 10\n",
    "sigma = 5\n",
    "\n",
    "x = np.linspace(-10, 30, 1000)\n",
    "pdf = gaussian_pdf(x, mu, sigma)\n",
    "\n",
    "samples = gaussian_sample(10000, mu, sigma)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, pdf)\n",
    "plt.hist(samples, bins=50, density=True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h202FnZAB_ne"
   },
   "source": [
    "# KL divergence\n",
    "KL divergence between two distributions $p, q$ is defined as\n",
    "$$ KL(p||q) = \\int p(z) \\log \\frac{p(z)}{q(z)} dz $$\n",
    "It tells us how two distribution differ from each other. Note that it's not simetrical so the order of $p$ and $q$ matters!  \n",
    "\n",
    "If both the distributions are Gaussian, we can express the KLD in a way easier form:\n",
    "\n",
    "$$ KL(p||q) = \\log\\frac{\\sigma_q}{\\sigma_p} + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac12 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtgV-OR8CD_A"
   },
   "outputs": [],
   "source": [
    "def kld(mu1, log_var1, mu2, log_var2):\n",
    "    return (\n",
    "        log_var2 - log_var1 + (np.exp(log_var1) + (mu1 - mu2) ** 2) / (np.exp(log_var2)) - 1\n",
    "    ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1718309794691,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "zcz_fvpoM5zS",
    "outputId": "cf9aa1aa-150d-4124-c817-cc37ee72080f"
   },
   "outputs": [],
   "source": [
    "mu1, log_var1 = 0, np.log(1)\n",
    "mu2, log_var2 = 0, np.log(4)\n",
    "kld_12 = kld(mu1, log_var1, mu2, log_var2)\n",
    "kld_21 = kld(mu2, log_var2, mu1, log_var1)\n",
    "\n",
    "kld_11 = kld(mu1, log_var1, mu1, log_var1)\n",
    "\n",
    "print(f'KL(p||q): {kld_12:.4f}')\n",
    "print(f'KL(q||p): {kld_21:.4f}')\n",
    "print(f'KL(p||p): {kld_11:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPj2Nbe53Ftq"
   },
   "source": [
    "# Generating 2D moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U-VDPrXwUkf"
   },
   "outputs": [],
   "source": [
    "def generate_moons(width=1.0):\n",
    "    moon1 = [\n",
    "        [r * np.cos(a) - 2.5, r * np.sin(a) - 1.0]\n",
    "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
    "        for a in np.arange(0, np.pi, 0.01)\n",
    "    ]\n",
    "    moon2 = [\n",
    "        [r * np.cos(a) + 2.5, r * np.sin(a) + 1.0]\n",
    "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
    "        for a in np.arange(np.pi, 2 * np.pi, 0.01)\n",
    "    ]\n",
    "    points = torch.tensor(moon1 + moon2)\n",
    "    points += torch.rand(points.shape) * width\n",
    "    return points.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_-2d0LYacVP"
   },
   "outputs": [],
   "source": [
    "class InMemDataLoader(object):\n",
    "    __initialized = False\n",
    "    def __init__(self, tensors, batch_size=1, shuffle=False, sampler=None,\n",
    "                 batch_sampler=None, drop_last=False):\n",
    "        \"\"\"A torch dataloader that fetches data from memory.\"\"\"\n",
    "        tensors = [torch.tensor(tensor) for tensor in tensors]\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if batch_sampler is not None:\n",
    "            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler option is mutually exclusive '\n",
    "                                 'with batch_size, shuffle, sampler, and '\n",
    "                                 'drop_last')\n",
    "            self.batch_size = None\n",
    "            self.drop_last = None\n",
    "\n",
    "        if sampler is not None and shuffle:\n",
    "            raise ValueError('sampler option is mutually exclusive with '\n",
    "                             'shuffle')\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    sampler = torch.utils.data.RandomSampler(dataset)\n",
    "                else:\n",
    "                    sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "            batch_sampler = torch.utils.data.BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.__initialized = True\n",
    "\n",
    "    def __setattr__(self, attr, val):\n",
    "        if self.__initialized and attr in ('batch_size', 'sampler', 'drop_last'):\n",
    "            raise ValueError('{} attribute should not be set after {} is '\n",
    "                             'initialized'.format(attr, self.__class__.__name__))\n",
    "\n",
    "        super(InMemDataLoader, self).__setattr__(attr, val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_indices in self.batch_sampler:\n",
    "            yield self.dataset[batch_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.dataset.tensors = tuple(t.to(device) for t in self.dataset.tensors)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1718309701991,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "rf1xiHEy2tj3",
    "outputId": "0618865f-7ecb-46ab-c8d1-f77906144200"
   },
   "outputs": [],
   "source": [
    "moons = generate_moons(width=1.0)\n",
    "moons_dl = InMemDataLoader([moons], batch_size=2048, shuffle=True)\n",
    "plt.scatter(moons[:, 0], moons[:, 1], s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpNiU4qgCyvG"
   },
   "source": [
    "\n",
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "VAE has two modules:\n",
    "1. Encoder encoding an initial data point $x$ into latent representation $z$. More precisely, the encoder returns $\\mu_z$ and $\\log \\sigma^2_z$ that are used to sample $z$ using reparametrization trick:\n",
    "$$ z = \\exp(\\log (\\sigma^2_z) / 2) * \\epsilon + \\mu_z $$\n",
    "where $\\epsilon \\sim N(0, I)$.\n",
    "2. Decoder decoding the latent $z$ into reconstructed $x\\_recon$.\n",
    "\n",
    "Training iteration contains the following steps:\n",
    "1. Sample data $x$ from the dataset.\n",
    "2. Use the Encoder to get $\\mu_z$ and $\\log \\sigma^2_z$.\n",
    "3. Use the reparametrization trick to sample $z$.\n",
    "4. Use the Decoder to reconstruct $z$ to $x\\_recon$.\n",
    "4. Compute loss:\n",
    "$$ L = MSE(x\\_recon, x) + KL\\big(N(\\mu_z, \\sigma_z^2) || N(0, I)\\big) $$\n",
    "\n",
    "For generation:\n",
    "1. Sample $z \\sim N(O, I)$.\n",
    "2. Use the Decoder to get $x\\_recon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "depZip50Gnfp"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim=2, hid_dim=128, z_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, 2 * z_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        sampled_z, z_mu, z_log_var = self.encode(x)\n",
    "        x_recon = self.decoder(sampled_z)\n",
    "        return x_recon, z_mu, z_log_var\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Implement encoding procedure.\n",
    "        # First, get z_mu and z_log_var from the encoder.\n",
    "        # Second, compute z samples using the  the reparametrization trick.\n",
    "        # TO!DO\n",
    "        z_mu_log_var = self.encoder(x)\n",
    "        z_mu, z_log_var = torch.chunk(z_mu_log_var, 2, dim=1)\n",
    "        # CUT{ sampled_z = TO!DO\n",
    "        epsilon = torch.randn(z_mu.shape).to(device)\n",
    "        sampled_z = epsilon * torch.exp(z_log_var / 2) + z_mu\n",
    "        # CUT}\n",
    "        return sampled_z, z_mu, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4m8bjOXLPCy"
   },
   "outputs": [],
   "source": [
    "def kld(mu1, log_var1, mu2, log_var2):\n",
    "    return (\n",
    "        log_var2 - log_var1 + (log_var1.exp() + (mu1 - mu2) ** 2) / (log_var2.exp()) - 1\n",
    "    ) / 2\n",
    "\n",
    "def kl_loss(z_mu, z_log_var):\n",
    "    kl_div = kld(z_mu, z_log_var, torch.zeros(1, device=device), torch.zeros(1, device=device))\n",
    "    return kl_div.sum() / z_mu.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o-ERw8E24yK"
   },
   "outputs": [],
   "source": [
    "hid_dim = 64\n",
    "z_dim = 2\n",
    "lr = 0.0003\n",
    "\n",
    "vae = VAE(hid_dim=hid_dim, z_dim=z_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 226954,
     "status": "error",
     "timestamp": 1718310821881,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "ZaTT2l073cQA",
    "outputId": "5514b759-cb95-4b92-a8ca-8aabf3bc7cc2"
   },
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    recon_loss_acc = 0.0\n",
    "    kl_acc = 0.0\n",
    "    vae.train()\n",
    "    for x, in moons_dl:\n",
    "        x = x.float().to(device)\n",
    "\n",
    "        x_recon, z_mu, z_log_var = vae(x)\n",
    "\n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        kl = kl_loss(z_mu, z_log_var)\n",
    "        loss = recon_loss + kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        recon_loss_acc += recon_loss.item() * len(x)\n",
    "        kl_acc += kl.item() * len(x)\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {i} loss: {(recon_loss_acc + kl_acc) / len(moons) :.4f} recon_loss: {recon_loss_acc / len(moons) :.4f} kl_loss: {kl_acc / len(moons) :.4f} avg mean: {z_mu.detach().mean() :.4f} avg std: {torch.exp(z_log_var.detach() / 2).mean() :.4f}\"\n",
    "        )\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            # Reconstruct data\n",
    "            x_recon = x_recon.cpu()\n",
    "\n",
    "            plt.scatter(x_recon[:, 0], x_recon[:, 1])\n",
    "            plt.title(\"Reconstruction\")\n",
    "            plt.show()\n",
    "\n",
    "            # Generate new data\n",
    "            z = torch.randn(500, z_dim).to(device)\n",
    "            x_gen = vae.decoder(z)\n",
    "            x_gen = x_gen.cpu()\n",
    "\n",
    "            plt.scatter(x_gen[:, 0], x_gen[:, 1])\n",
    "            plt.title(\"Generation\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NIA-xRP9mim"
   },
   "outputs": [],
   "source": [
    "def get_grid(data):\n",
    "    \"\"\"Generate a dataset of points that lie on grid and span the given data range.\"\"\"\n",
    "\n",
    "    xmin, xmax = np.floor(data.min(0)), np.ceil(data.max(0))\n",
    "    xg, yg = np.meshgrid(\n",
    "        np.arange(xmin[0], xmax[0] + 1, 1), np.arange(xmin[1], xmax[1] + 1, 1)\n",
    "    )\n",
    "    mxg = np.hstack(\n",
    "        (\n",
    "            np.hstack((xg, np.zeros((xg.shape[0], 1)) + np.nan)).ravel(),\n",
    "            np.hstack((xg.T, np.zeros((xg.shape[1], 1)) + np.nan)).ravel(),\n",
    "        )\n",
    "    )\n",
    "    myg = np.hstack(\n",
    "        (\n",
    "            np.hstack((yg, np.zeros((yg.shape[0], 1)) + np.nan)).ravel(),\n",
    "            np.hstack((yg.T, np.zeros((yg.shape[1], 1)) + np.nan)).ravel(),\n",
    "        )\n",
    "    )\n",
    "    grid = np.vstack((mxg, myg)).T\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1718311054037,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "6mRtRod68qkL",
    "outputId": "21ce008b-a449-4014-ea5b-fe23efd5bf7d"
   },
   "outputs": [],
   "source": [
    "data = np.array(moons)[np.random.permutation(moons.shape[0])[:1000]]\n",
    "grid = get_grid(data)\n",
    "\n",
    "data_colors = (data[:, 0] - min(data[:, 0])) / (max(data[:, 0]) - min(data[:, 0]))\n",
    "data_colors = plt.cm.jet(data_colors)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(grid[:, 0], grid[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(data[:, 0], data[:, 1], color=data_colors, s=1.0)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Data in original space\")\n",
    "\n",
    "vae.eval()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    enc_grid, _, _ = vae.encode(\n",
    "        torch.from_numpy(grid).to(device).float()\n",
    "    )\n",
    "    enc_data, _, _ = vae.encode(\n",
    "        torch.from_numpy(data).to(device).float()\n",
    "    )\n",
    "enc_grid = enc_grid.cpu().numpy()\n",
    "enc_data = enc_data.cpu().numpy()\n",
    "\n",
    "plt.plot(enc_grid[:, 0], enc_grid[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(enc_data[:, 0], enc_data[:, 1], color=data_colors, s=1.0)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Data in latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1718311083194,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "gB_zss4NABEK",
    "outputId": "8cbd9462-0cc8-407c-a383-f51f01883768"
   },
   "outputs": [],
   "source": [
    "latent_samples = torch.randn(1000, z_dim)\n",
    "\n",
    "latent_colors = (latent_samples[:, 0] - min(latent_samples[:, 0])) / (\n",
    "    max(latent_samples[:, 0]) - min(latent_samples[:, 0])\n",
    ")\n",
    "latent_colors = plt.cm.jet(latent_colors.numpy())\n",
    "\n",
    "latent_grid = get_grid(latent_samples.numpy())\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    x_gen = vae.decoder(latent_samples.to(device))\n",
    "    x_gen = x_gen.cpu()\n",
    "\n",
    "    grid_gen = vae.decoder(\n",
    "        torch.from_numpy(latent_grid).float().to(device)\n",
    "    )\n",
    "    grid_gen = grid_gen.cpu()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(latent_grid[:, 0], latent_grid[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(latent_samples[:, 0], latent_samples[:, 1], color=latent_colors, s=1)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Z in latent space\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(grid_gen[:, 0], grid_gen[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(x_gen[:, 0], x_gen[:, 1], color=latent_colors, s=1)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Generated data in original space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yosfsSTGhyw"
   },
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "GAN is built with two networks:\n",
    "1. Discriminator judging if a given sample is real or no.\n",
    "2. Generator creating samples from initial noise.\n",
    "\n",
    "They plan a min-max game with each other. The Generator tries to generate samples that are hard to distinguish from real ones by the Discriminator. Meanwhile, the Discriminator tries to spot the difference between real and fake samples.\n",
    "\n",
    "Training looks as follows:\n",
    "1. Get data point $x$.\n",
    "2. Sample initial noise $z \\sim N(0, I)$.\n",
    "3. Generate $x\\_fake$ from $z$ using the Generator.\n",
    "4. Do binary classification on $x$ and $x\\_fake$ with the Discrimator.\n",
    "5. Compute loss for the Generator:\n",
    "$$ L_G = \\log(D(G(z)))$$\n",
    "6. Compute loss for the Discrimator:\n",
    "$$ L_D = \\log (D(x)) + \\log(1 - D(G(z))) $$\n",
    "\n",
    "To generate new data:\n",
    "1. Sample initial noise $z \\sim N(0, I)$.\n",
    "2. Generate $x\\_fake$ from $z$ using the Generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz36Jq6hQzqs"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim=2, hid_dim=128, out_dim=2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim=2, hid_dim=128, out_dim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, out_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsJT3DFWAzmY"
   },
   "outputs": [],
   "source": [
    "def generator_loss(DG, eps=1e-6):\n",
    "    # Define Generator loss. Use eps for numerical stability of log.\n",
    "    # CUT{ return TO!DO\n",
    "    loss = torch.log(DG + eps)  # loss = TO!DO\n",
    "    return -torch.mean(loss)\n",
    "    # CUT}\n",
    "\n",
    "\n",
    "def discriminator_loss(DR, DG, eps=1e-6):\n",
    "    # Define Discriminator loss. Use eps for numerical stability of log.\n",
    "    # CUT{ return\n",
    "    loss = torch.log(DR + eps) + torch.log(1 - DG + eps)  # loss = TO!DO\n",
    "    return -torch.mean(loss)\n",
    "    # CUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXMleqaUA5FQ"
   },
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "hid_dim = 64\n",
    "lr = 0.0001\n",
    "\n",
    "G = Generator(in_dim=z_dim, hid_dim=hid_dim).to(device)\n",
    "D = Discriminator(hid_dim=hid_dim).to(device)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1gwQqpfuKR21TgUSavvYSxBlfRlhUJyzl"
    },
    "executionInfo": {
     "elapsed": 539105,
     "status": "ok",
     "timestamp": 1718311905497,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "2ggBMd88D_By",
    "outputId": "f3c1912c-c362-49d0-f31a-bdb087fb3ab9"
   },
   "outputs": [],
   "source": [
    "for i in range(4500):\n",
    "    G_loss_acc = 0.0\n",
    "    D_loss_acc = 0.0\n",
    "    G.train()\n",
    "    D.train()\n",
    "    for x, in moons_dl:\n",
    "        x = x.float().to(device)\n",
    "\n",
    "        # Generate fake data from z ~ N(0,1).\n",
    "        # Calculate Generator loss.\n",
    "        z = torch.randn(x.size(0), z_dim, device=device)\n",
    "        # CUT{ x_fake = TO!DO  # Use the generator to compute x_Fake\n",
    "        x_fake = G(z)\n",
    "        # CUT}\n",
    "\n",
    "        # make a copy of x_fake and detach it, we'll use the copy to train the Discriminator\n",
    "        x_fake_detached = x_fake.detach()\n",
    "\n",
    "        # CUT{ G_loss = TO!DO  # Now use the discriminator and compute generator loss\n",
    "        G_loss = generator_loss(D(x_fake))\n",
    "        # CUT}\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Calculate Discriminator loss.\n",
    "        # Remember to use x_fake_detached to prevent backpropagating through generator!\n",
    "        # CUT{ D_loss=\n",
    "        D_loss = discriminator_loss(D(x), D(x_fake_detached))\n",
    "        # CUT}\n",
    "\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        G_loss_acc += G_loss.item() * len(x)\n",
    "        D_loss_acc += D_loss.item() * len(x)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(1000, z_dim, device=device)\n",
    "            x_gen = G(z).cpu()\n",
    "            plt.scatter(x_gen[:, 0], x_gen[:, 1])\n",
    "            plt.title(\n",
    "                f\"Epoch: {i} Generator loss: {G_loss_acc / len(moons) :.4f} Discriminator loss: {D_loss_acc / len(moons) :.4f}\"\n",
    "            )\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ul1Q829FNk8E",
    "outputId": "4a9db9e6-a2ae-4f08-b10e-4523537cf909"
   },
   "outputs": [],
   "source": [
    "latent_samples = torch.randn(1000, z_dim)\n",
    "\n",
    "latent_colors = (latent_samples[:, 0] - min(latent_samples[:, 0])) / (\n",
    "    max(latent_samples[:, 0]) - min(latent_samples[:, 0])\n",
    ")\n",
    "latent_colors = plt.cm.jet(latent_colors.numpy())\n",
    "\n",
    "latent_grid = get_grid(latent_samples.numpy())\n",
    "\n",
    "G.eval()\n",
    "# !: compute the projection into data space of the latent saples and the grid\n",
    "# CUT{ x_gen = TO!DO\\ngrid_gen = TO!DO\n",
    "with torch.no_grad():\n",
    "    x_gen = G(latent_samples.to(device)).cpu()\n",
    "    grid_gen = G(torch.from_numpy(latent_grid).float().to(device)).cpu()\n",
    "# CUT}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(latent_grid[:, 0], latent_grid[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(latent_samples[:, 0], latent_samples[:, 1], color=latent_colors, s=1)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Z in latent space\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(grid_gen[:, 0], grid_gen[:, 1], color=\"gray\", alpha=0.3)\n",
    "plt.scatter(x_gen[:, 0], x_gen[:, 1], color=latent_colors, s=1)\n",
    "_ = plt.axis(\"equal\")\n",
    "plt.title(\"Generated data in original space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMEXkghZP5LF"
   },
   "source": [
    "# Diffusion model\n",
    "\n",
    "Diffusion models can be seen as a combination of hierarchical VAE where there are multiple hidden states and normalizing flows where the goal is to learn invertible mapping from data distribution to Gaussian one.\n",
    "  \n",
    "  For a given schedule of noise level $\\{\\beta_t\\}_{t=1}^T$, we can define two processes:\n",
    "  1. *Forward process* that gradually adds small noise to the initial image $x_0$, ending up in a normally-distributed sample $x_T$, following the equation:\n",
    "  $$q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = N(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})$$\n",
    "  where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$.\n",
    "\n",
    "  2. *Backward process* used during inference to remove noise from initial Gaussian sample $x_T$ up until a clean image $x_0$, following the $p(x_{t-1}|x_t)$ distribution.\n",
    "\n",
    "  The general goal is to learn the $p(x_{t-1}|x_t)$ distribution. However, it is intractable and we just assume it's Gaussian. Then, we do something similar to VAE, and look at the variational posterior $q(x_{t-1} | x_t, x_0)$ that is also Gaussian and is tractable.\n",
    "\n",
    "\n",
    "We can summarize the training in the following steps:\n",
    "  1. Sample timesteps for the batch of $x_0$.\n",
    "  2. Add noise according to $q(\\mathbf{x}_t \\vert \\mathbf{x}_0)$.\n",
    "  3. Predict noise using denoiser backbone from $x_t$ and timestep.\n",
    "  4. Calculate MSE loss between the true noise and predicted one.\n",
    "\n",
    "And finally the inference:\n",
    "  1. Sample $x_T$ from Gaussian distribution.\n",
    "  2. Loop over timesteps in reversed order:\n",
    "    1. Predict noise from the current timestep and $x_t$.\n",
    "    2. Get parameters for $p(x_{t-1}|x_t)$: mean from $q(x_{t-1} | x_t, x_0)$ and a fixed variance.\n",
    "    3. Sample $x_{t-1}$ from $p(x_{t-1}|x_t)$.\n",
    "\n",
    "Note: There are a couple of simplifications used in this exercise (justified in the literature as well):\n",
    "  1. We use MSE as a loss, omitting ELBO.\n",
    "  2. We don't learn the variance of $p(x_{t-1}|x_t)$. Instead, we just use a fixed schedule for it.\n",
    "  3. Since we're working with moons dataset, we don't need to discritize $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7NwgLjoP9l8"
   },
   "outputs": [],
   "source": [
    "class ConditionalLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_timesteps):\n",
    "        super(ConditionalLinear, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.embed = nn.Embedding(n_timesteps, out_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.fc(x)\n",
    "        t_emb = self.embed(t)\n",
    "        x = t_emb.reshape(-1, self.out_dim) * x\n",
    "        return x\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, data_dim, hid_dim, n_timesteps):\n",
    "        super(Denoiser, self).__init__()\n",
    "\n",
    "        self.cl1 = ConditionalLinear(data_dim, hid_dim, n_timesteps)\n",
    "        self.cl2 = ConditionalLinear(hid_dim, hid_dim, n_timesteps)\n",
    "        self.cl3 = nn.Linear(hid_dim, data_dim)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.activation(self.cl1(x, t))\n",
    "        x = self.activation(self.cl2(x, t))\n",
    "        return self.cl3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbMJxV9UQMRn"
   },
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "  def __init__(self, denoiser, n_timesteps=10, data_dim=2, beta_start=1e-5, beta_end=0.9):\n",
    "    super(Diffusion, self).__init__()\n",
    "    self.denoiser = denoiser\n",
    "    self.n_timesteps = n_timesteps\n",
    "    self.data_dim = data_dim\n",
    "\n",
    "    self.beta = nn.Parameter(torch.linspace(beta_start, beta_end, n_timesteps), requires_grad=False)\n",
    "    self.set_params()\n",
    "\n",
    "  def forward(self, x0):\n",
    "    timesteps = torch.randint(self.n_timesteps, (x0.shape[0],)).to(x0.device)\n",
    "    eps, xt = self.get_noisy_sample(x0, timesteps)\n",
    "\n",
    "    eps_pred = self.denoiser(xt, timesteps)\n",
    "\n",
    "    loss = F.mse_loss(eps_pred, eps)\n",
    "    return loss\n",
    "\n",
    "  def sample(self, batch_size=1):\n",
    "    with torch.no_grad():\n",
    "      xt = torch.randn(batch_size, self.data_dim).to(self.device)\n",
    "\n",
    "      for t in reversed(range(self.n_timesteps)):\n",
    "        timesteps = torch.tensor([t] * xt.shape[0]).to(xt.device)\n",
    "        eps_pred = self.denoiser(xt, timesteps)\n",
    "        mean, logvar = self.get_p_params(xt, timesteps, eps_pred)\n",
    "        noise = torch.randn_like(xt) #if t > 0 else torch.zeros_like(xt)\n",
    "        xt = mean + noise * torch.exp(logvar / 2)\n",
    "\n",
    "    return xt\n",
    "\n",
    "  def get_p_params(self, xt, timesteps, eps_pred):\n",
    "    # we use fixed variance schedule for p(x_{t-1} | x_t)\n",
    "    p_logvar = self.broadcast(torch.log(self.beta[timesteps]), dim=xt.ndim)\n",
    "\n",
    "    # get mean for p(x_{t-1} | x_t)\n",
    "    p_mean = self.get_q_params(xt, timesteps, eps_pred)\n",
    "    return p_mean, p_logvar\n",
    "\n",
    "  def get_q_params(self, xt, timesteps, eps_pred):\n",
    "    # TO!DO\n",
    "    # predict x0 from xt and eps_pred\n",
    "    coef1_x0 = self.broadcast(self.coef1_x0[timesteps], dim=xt.ndim)\n",
    "    coef2_x0 = self.broadcast(self.coef2_x0[timesteps], dim=xt.ndim)\n",
    "    x0 = coef1_x0 * xt - coef2_x0 * eps_pred\n",
    "\n",
    "    # TO!DO\n",
    "    # q(x_{t-1} | x_t, x_0)\n",
    "    coef1_q = self.broadcast(self.coef1_q[timesteps], dim=xt.ndim)\n",
    "    coef2_q = self.broadcast(self.coef2_q[timesteps], dim=xt.ndim)\n",
    "    q_mean = coef1_q * x0 + coef2_q * xt\n",
    "\n",
    "    return q_mean\n",
    "\n",
    "  def get_noisy_sample(self, x0, timesteps):\n",
    "    # TO!DO\n",
    "    # sample from q(xt | x0)\n",
    "    eps = torch.randn_like(x0)\n",
    "    xt = self.broadcast(torch.sqrt(self.alpha_bar[timesteps]), dim=x0.ndim) * x0 + self.broadcast(torch.sqrt(1 - self.alpha_bar[timesteps]), dim=x0.ndim) * eps\n",
    "    return eps, xt\n",
    "\n",
    "  def set_params(self):\n",
    "    # helper method for all of the constants needed\n",
    "    self.alpha = nn.Parameter(1 - self.beta, requires_grad=False)\n",
    "    self.alpha_bar = nn.Parameter(torch.cumprod(self.alpha, dim=0), requires_grad=False)\n",
    "    self.alpha_bar_prev = nn.Parameter(torch.cat([torch.ones(1,), self.alpha_bar[:-1]]), requires_grad=False)\n",
    "\n",
    "    # to caluclate x0 from eps_pred\n",
    "    self.coef1_x0 = nn.Parameter(torch.sqrt(1.0 / self.alpha_bar), requires_grad=False)\n",
    "    self.coef2_x0 = nn.Parameter(torch.sqrt(1.0 / self.alpha_bar - 1), requires_grad=False)\n",
    "\n",
    "    # for q(x_{t-1} | x_t, x_0)\n",
    "    self.coef1_q = nn.Parameter(self.beta * torch.sqrt(self.alpha_bar_prev) / (1.0 - self.alpha_bar), requires_grad=False)\n",
    "    self.coef2_q = nn.Parameter((1.0 - self.alpha_bar_prev) * torch.sqrt(self.alpha) / (1.0 - self.alpha_bar), requires_grad=False)\n",
    "\n",
    "  def broadcast(self, arr, dim=2):\n",
    "    # helper method to increase tensor's dimension number\n",
    "    while arr.dim() < dim:\n",
    "        arr = arr[:, None]\n",
    "    return arr.to(self.device)\n",
    "\n",
    "  @property\n",
    "  def device(self):\n",
    "    return next(self.denoiser.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XtlscX5OSYW"
   },
   "outputs": [],
   "source": [
    "hid_dim = 128\n",
    "data_dim = 2\n",
    "n_timesteps = 10\n",
    "lr = 0.001\n",
    "\n",
    "denoiser = Denoiser(data_dim, hid_dim, n_timesteps)\n",
    "diffusion = Diffusion(denoiser, n_timesteps=n_timesteps, data_dim=data_dim).to(device)\n",
    "optimizer = optim.Adam(diffusion.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NDVglgxP5XZ"
   },
   "outputs": [],
   "source": [
    "timesteps = torch.arange(n_timesteps)\n",
    "x0 = next(iter(moons_dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3368,
     "status": "ok",
     "timestamp": 1718312389222,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "WFWRdO4mWna1",
    "outputId": "6d1cf15c-831f-421c-edeb-efcea28326e4"
   },
   "outputs": [],
   "source": [
    "for t in timesteps[::1]:\n",
    "  t = torch.zeros(x0.shape[0]) + t\n",
    "  xt = diffusion.get_noisy_sample(x0, t.int())[1]\n",
    "  xt = xt.cpu()\n",
    "  plt.figure()\n",
    "  plt.xlim([-9, 9])\n",
    "  plt.ylim([-6, 6])\n",
    "  plt.scatter(xt[:, 0], xt[:, 1])\n",
    "  plt.title(f'Noisy samples for timestep {t[0].int()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 219704,
     "status": "ok",
     "timestamp": 1718312619862,
     "user": {
      "displayName": "Michał Stypułkowski",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "7_Cm7VVVOWWL",
    "outputId": "325e4f2d-b301-4719-bdb5-7d981f0bb799"
   },
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    loss_acc = 0.0\n",
    "    diffusion.train()\n",
    "    for x0, in moons_dl:\n",
    "      x0 = x0.float()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss = diffusion(x0)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_acc += loss.item()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "      diffusion.eval()\n",
    "      with torch.no_grad():\n",
    "        samples = diffusion.sample(2048)\n",
    "        samples = samples.cpu()\n",
    "\n",
    "        plt.scatter(samples[:, 0], samples[:, 1])\n",
    "        plt.title(f\"Epoch: {i} loss: {loss_acc / len(moons_dl) :.4f}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6mQOEmrnSIA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
