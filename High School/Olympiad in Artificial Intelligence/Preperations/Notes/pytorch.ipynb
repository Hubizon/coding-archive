{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torchvision import transforms\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from IPython.display import clear_output\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.tensor(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug in google colab\n",
    "\n",
    "import pdb\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear layer\n",
    "\n",
    "lr = nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "print(lr.parameters())\n",
    "print(lr.state_dict())\n",
    "print(lr.weight)\n",
    "print(lr.bias)\n",
    "print(lr(torch.tensor([[2.], [3.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression model\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LR(1, 1)\n",
    "print(model(torch.tensor([[2.], [3.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a simple linear regression\n",
    "\n",
    "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "y = -3 * X + 5\n",
    "y += torch.randn(y.size())\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "def criterion(pred, y):\n",
    "    return torch.mean((pred - y) ** 2)\n",
    "\n",
    "w = torch.tensor(10., requires_grad=True)\n",
    "b = torch.tensor(-1., requires_grad=True)\n",
    "lr, n_epochs = 0.1, 10\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    pred = forward(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    # w.data -= lr * w.grad.data # alternatively\n",
    "    # print(loss.item())\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch training with data loader\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "        self.y = 2 * self.x - 3\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "w = torch.tensor(10., requires_grad=True)\n",
    "b = torch.tensor(10., requires_grad=True)\n",
    "\n",
    "dataset = Data()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4)\n",
    "def train(n_epochs):\n",
    "    global w, b\n",
    "    for _ in range(n_epochs):\n",
    "        for X, y in dataloader:\n",
    "            pred = forward(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                w -= lr * w.grad\n",
    "                b -= lr * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "\n",
    "train(5)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with an optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = LR(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "trainloader = DataLoader(dataset=dataset, batch_size=16)\n",
    "\n",
    "def train(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for X, y in trainloader:\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "train(5)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with he initialized weights (for relu) + xavier + dropout + batch norm\n",
    "# (dropout can sometimes be before the activation)\n",
    "# (batch normalization: really big batch sizes, no dropout)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 10)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(10) # num of layer's outputs\n",
    "        nn.init.kaiming_uniform_(self.linear1.weight, nonlinearity='relu')\n",
    "        \n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.drop(self.relu(self.linear1(x)))\n",
    "        x = self.batch_norm1(self.relu(self.linear1(x)))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(model(torch.tensor([[1.], [3.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting anomalies\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# sample data\n",
    "normal_data = np.random.normal(2, 1, (1000, 50))\n",
    "anomalous_data = np.random.normal(0, 2, (50, 50))\n",
    "data = torch.tensor(np.vstack([normal_data, anomalous_data]), dtype=torch.float, device=device)\n",
    "data_loader = DataLoader(TensorDataset(data), batch_size=32, shuffle=True)\n",
    "\n",
    "# training\n",
    "model = Autoencoder(data.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(11):\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"{epoch}: loss: {loss.item():.4f}\")\n",
    "\n",
    "# detecting anomalies\n",
    "def detect_anomaly(data, threshold):\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data)\n",
    "    return loss.item() > threshold\n",
    "\n",
    "def detect_anomalies(data, threshold):\n",
    "    #data = torch.tensor(data, dtype=torch.float32)\n",
    "    outputs = model(data)\n",
    "    losses = nn.functional.mse_loss(outputs, data, reduction='none').mean(dim=1)\n",
    "    anomalies = losses > threshold\n",
    "    return anomalies\n",
    "\n",
    "threshold = 2.0 \n",
    "\n",
    "anomalies = []\n",
    "for i, row in enumerate(data):\n",
    "    if detect_anomaly(row, threshold):\n",
    "        anomalies.append(i)\n",
    "\n",
    "print(f'Anomalies detected at indices: {anomalies}')\n",
    "\n",
    "preds = detect_anomalies(data, threshold).cpu()\n",
    "y = np.full(1050, False)\n",
    "y[1000:] = True\n",
    "print(\"accuracy: \", accuracy_score(y, preds))\n",
    "print(\"precision: \", precision_score(y, preds))\n",
    "print(\"recall: \", recall_score(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores of thresholds\n",
    "\n",
    "thresholds = np.arange(0.05, 5.0, 0.05)\n",
    "score_acc = []\n",
    "score_prec = []\n",
    "score_rec = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = detect_anomalies(data, threshold).cpu()\n",
    "    score_acc.append(accuracy_score(y, preds))\n",
    "    score_prec.append(precision_score(y, preds))\n",
    "    score_rec.append(recall_score(y, preds))\n",
    "\n",
    "plt.plot(thresholds, score_acc, label='accuracy_score')\n",
    "plt.plot(thresholds, score_prec, label='precision_score')\n",
    "plt.plot(thresholds, score_rec, label='recall_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "x_sample = torch.rand((10_000, 1), device=device)\n",
    "y_sample = (4*x_sample**5 + -3*x_sample**2) > -0.3\n",
    "train_dataset = TensorDataset(x_sample[:-1000], y_sample[:-1000].type(torch.float))\n",
    "valid_dataset = TensorDataset(x_sample[-1000:], y_sample[-1000:].type(torch.float))\n",
    "train_loader = DataLoader(train_dataset, batch_size=512)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to train a model\n",
    "# you may need to convert tensors to float32 (before criterium)\n",
    "# and .to(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "\n",
    "def compute_error(model, data_loader, criterion, c_sum=False):\n",
    "    model.eval()\n",
    "    losses, num_of_el = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if not c_sum: loss *= len(y)\n",
    "            losses += loss\n",
    "            num_of_el += len(y)\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "              train_loader: DataLoader,\n",
    "              valid_loader: DataLoader,\n",
    "              num_epochs: int,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion,\n",
    "              verbose: bool = True,\n",
    "              verbose_plot: bool = False\n",
    "              ) -> float:\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "            _iter += 1\n",
    "\n",
    "        val_loss = compute_error(model, valid_loader, criterion)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "        if verbose_plot:\n",
    "            train_loss = compute_error(model, train_loader, criterion)\n",
    "            train_losses.append(train_loss.detach().cpu())\n",
    "            valid_losses.append(val_loss.detach().cpu())\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    if verbose_plot:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(train_losses, c='b', label='train')\n",
    "        plt.plot(valid_losses, c='r', label='valid')\n",
    "        plt.grid(ls=':')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a single model\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_model(model, train_loader, valid_loader, 10, optimizer, criterion, verbose=True, verbose_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into k folds and train k models on them\n",
    "\n",
    "def train_kfold(Net, dataset, n_splits=5, num_epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    models = nn.ModuleList()\n",
    "    scores = []\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    for train_ids, val_ids in kf.split(dataset):\n",
    "        train_sub = Subset(dataset, train_ids)\n",
    "        valid_sub = Subset(dataset, val_ids)\n",
    "        train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = Net().to(device)\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        train_model(model, train_loader, valid_loader, num_epochs=num_epochs, \n",
    "                    optimizer=optimizer, criterion=criterion, verbose=True)\n",
    "\n",
    "        scores.append(compute_error(model, valid_loader, criterion).detach().cpu())\n",
    "        models.append(model)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "# training the models and checking the scores\n",
    "models, scores = train_kfold(Net, train_dataset, n_splits=10, num_epochs=1, batch_size=256)\n",
    "clear_output(False)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to predict outputs with k models\n",
    "\n",
    "def predict_ensemble(models, x):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model_preds = []\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            model_preds.append(pred)\n",
    "        predictions.append(torch.cat(model_preds))\n",
    "\n",
    "    predictions = torch.mean(torch.stack(predictions), dim=0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, data_loader, criterion, c_sum=False):\n",
    "    losses, num_of_el = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = predict_ensemble(models, x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if not c_sum: loss *= len(y)\n",
    "            losses += loss\n",
    "            num_of_el += len(y)\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "print(evaluate_ensemble(models, valid_loader, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy on a single and multiple models\n",
    "\n",
    "# binary output\n",
    "def accuracy_binary(outputs, y, threshold=0.5):\n",
    "    pred = outputs > threshold\n",
    "    return sum(pred == y)\n",
    "\n",
    "# multiple outputs\n",
    "def accuracy_multiple(outputs, y):\n",
    "    pred = outputs.argmax(dim=1)\n",
    "    return sum(pred == y)\n",
    "\n",
    "print(compute_error(models[0], valid_loader, accuracy_binary, c_sum=True))\n",
    "print(evaluate_ensemble(models, valid_loader, accuracy_binary, c_sum=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using torchvision\n",
    "\n",
    "model = torchvision.models.resnet50(weights='ResNet50_Weights.DEFAULT').to(device)\n",
    "\n",
    "# normalize data - same for every torchvision pretrained model !\n",
    "# and an example of some other transforms\n",
    "composed = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change the last layer to fit our needs\n",
    "# (you can check how does your chosen network looks like first [model])\n",
    "num_classes = 3\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes).to(device)\n",
    "\n",
    "# optimize the parameters of the last layer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "# train_model(...)\n",
    "\n",
    "# prediction\n",
    "img = PIL.Image.fromarray((np.random.rand(224, 224, 3) * 255).astype(np.uint8))\n",
    "model(composed(img)[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a CNN (28x28x3 -> 10 classes)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 11 * 11, 128),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear_layers(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "model(torch.zeros((1, 3, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a CNN with batch normalization (16x16x1 -> 10 classes)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, stride=1, padding=\"same\")\n",
    "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=\"same\")\n",
    "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "        self.fc1_bn = nn.BatchNorm1d(10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1_bn(torch.relu(self.cnn1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2_bn(torch.relu(self.cnn2(x)))\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1_bn(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model(torch.rand((3, 1, 16, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a convolutional neural network with skip connections\n",
    "\n",
    "class ConvSkipNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvSkipNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(16 * 16 * 32, 10) \n",
    "\n",
    "        self.skip_conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip_conv(x) \n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out) + identity) \n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvSkipNet()\n",
    "input_data = torch.randn(2, 3, 16, 16)\n",
    "print(model(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM for explainability \n",
    "\n",
    "# similar to previous cell with torchvision + defining dataloader\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "composed = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root='./data/imgs/', transform=composed)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "\n",
    "# defining our model with gradient hooks\n",
    "# densenet: https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82#:~:text=case%20of%20the-,DenseNet,-)%20we%20are%20going\n",
    "class Net_Grad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # check the vgg19 architecture first\n",
    "        self.model = torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1')\n",
    "        self.features_conv = self.model.features[:36]\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "        self.classifier = self.model.classifier\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)\n",
    "\n",
    "\n",
    "# getting the heatmap\n",
    "model = Net_Grad()\n",
    "model.eval()\n",
    "img, _ = next(iter(dataloader))\n",
    "preds = model(img)\n",
    "pred = preds.argmax(dim=1)\n",
    "\n",
    "preds[:, pred].backward()\n",
    "gradients = model.get_activations_gradient()\n",
    "pooled_gradient = torch.mean(gradients, dim=[0, 2, 3])\n",
    "activations = model.get_activations(img).detach()\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradient[i]\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= torch.max(heatmap)\n",
    "plt.matshow(heatmap.squeeze())\n",
    "\n",
    "\n",
    "# interpolate the heat-map onto the image \n",
    "image = PIL.Image.open('./data/imgs/elephant/elephant.jpg').convert('RGBA')\n",
    "heatmap_colored = (plt.cm.jet(heatmap) * 255).astype(np.uint8)\n",
    "heatmap_image = PIL.Image.fromarray(heatmap_colored).resize(image.size, PIL.Image.BILINEAR)\n",
    "blended_image = PIL.Image.blend(image, heatmap_image, alpha=0.5)\n",
    "blended_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple recurrent net with one output for each input\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn1 = nn.LSTM(500, 128, batch_first=False, num_layers=3, bidirectional=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        packed_inputs = pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, _ = self.rnn1(packed_inputs)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        outputs = self.layers(outputs)[:, -1, :] # if you want only the last state\n",
    "        return outputs\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "x = torch.randn(3, 5, 500, device=device)  # (batch_size, seq_len, input_size)\n",
    "input_lengths = torch.randint(1, 5, (3,))\n",
    "model(x, input_lengths), input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a recurrent net with varying outputs\n",
    "\n",
    "def loss_fn(output, target, mask):\n",
    "    # mask, target, outputs - tensors with the same shape\n",
    "    output_masked = output[mask]\n",
    "    target_masked = target[mask]\n",
    "    loss = F.mse_loss(output_masked, target_masked)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features=500, n_outputs=1):\n",
    "        super().__init__()\n",
    "        self.rnn1 = nn.LSTM(n_features, 128, batch_first=False, num_layers=3, bidirectional=True)\n",
    "        self.rnn2 = nn.RNN(256, 256, batch_first=False)\n",
    "        self.rnn3 = nn.GRU(256, 128, batch_first=False, bidirectional=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(128),\n",
    "            nn.Linear(128, n_outputs)\n",
    "        )\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.layers[0].weight, nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        packed_inputs = pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, _ = self.rnn1(packed_inputs)\n",
    "        packed_outputs, _ = self.rnn2(packed_outputs)\n",
    "        packed_outputs, _ = self.rnn3(packed_outputs)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        outputs = self.layers(outputs)\n",
    "        outputs = outputs[:, :, :len(packed_inputs.batch_sizes)]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "model = Net(n_outputs=5)\n",
    "model.to(device)\n",
    "\n",
    "# (batch_size, seq_len, input_size)\n",
    "sample_data = torch.randn((3, 5, 500), device=device)\n",
    "sample_lengths = torch.randint(low=1, high=5, size=(3,))\n",
    "\n",
    "# output: max input cnt in batch ^2\n",
    "# for each sample in a batch: input cnt ^2 + padding\n",
    "model(sample_data, sample_lengths), sample_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series dataset for training recurrent networks\n",
    "# (const seq_length and predicting next var in time series)\n",
    "\n",
    "# dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_length]\n",
    "        y = self.data[idx+self.seq_length]\n",
    "        return torch.tensor(x, dtype=torch.float, device=device), torch.tensor(y, dtype=torch.float, device=device)\n",
    "\n",
    "# model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features=500, n_outputs=1):\n",
    "        super().__init__()\n",
    "        self.rnn1 = nn.LSTM(n_features, 128, batch_first=True, num_layers=1)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(128),\n",
    "            nn.Linear(128, n_outputs)\n",
    "        )\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.layers[0].weight, nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn1(x)\n",
    "        x = x[:, -1, :] # if you want only the last state\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "# some sample data\n",
    "data = torch.linspace(0, 100, 200).reshape(-1, 2)\n",
    "\n",
    "# dataset and data loader\n",
    "seq_length = 5\n",
    "dataset = TimeSeriesDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# sample training loop\n",
    "model = Net(n_outputs=4, n_features=2).to(device)\n",
    "for x, y in data_loader:\n",
    "    outputs = model(x)\n",
    "    break\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple word2vec model with gensim\n",
    "\n",
    "# pretrained\n",
    "w2v = api.load('glove-wiki-gigaword-50')\n",
    "word_embedding = w2v['king']\n",
    "print(word_embedding[:7])\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    [\"cat\", \"say\", \"meow\"],\n",
    "    [\"dog\", \"say\", \"woof\", \"woof\"],\n",
    "    [\"cat\", \"chase\", \"mouse\"],\n",
    "    [\"dog\", \"chase\", \"cat\"]\n",
    "]\n",
    "\n",
    "# window: maximum distance between the current and predicted word within a sentence\n",
    "# min_count: Ignores all words with total frequency lower than this\n",
    "# workers: Number of worker threads to use\n",
    "model = Word2Vec(sentences, vector_size=3, window=5, min_count=1, workers=-1)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=100)\n",
    "\n",
    "embed_vector = model.wv['cat']\n",
    "print(embed_vector, embed_vector.shape)\n",
    "\n",
    "most_similar = model.wv.most_similar('cat', topn=3)\n",
    "print(most_similar)\n",
    "\n",
    "similarity_cat_dog = model.wv.similarity('cat', 'dog')\n",
    "print(similarity_cat_dog)\n",
    "\n",
    "distance_cat_dog = model.wv.distance('cat', 'dog')\n",
    "print(distance_cat_dog)\n",
    "\n",
    "words_similar_by_vector = model.wv.similar_by_vector(embed_vector, topn=3)\n",
    "print(words_similar_by_vector)\n",
    "\n",
    "odd_one_out = model.wv.doesnt_match(['cat', 'dog', 'mouse', 'say'])\n",
    "print(odd_one_out)\n",
    "\n",
    "result = model.wv.most_similar(positive=['dog', 'meow'], negative=['cat'], topn=3)\n",
    "print(result)\n",
    "\n",
    "vocabulary = list(model.wv.index_to_key)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face transformers library \n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "result = classifier(\"The actors were very convincing\")\n",
    "print(result)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"huggingface/distilbert-base-uncased-finetuned-mnli\")\n",
    "print(classifier(\"She loves me. [SEP] She loves me not.\"))\n",
    "\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "result = generator(\"Once upon a time in a land far, far away\", max_length=50, truncation=True, num_return_sequences=3)\n",
    "print(result)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "    \"Joe lived for a very long time. [SEP] Joe is old.\"], padding=True, return_tensors='pt')\n",
    "print(token_ids['input_ids'])\n",
    "print(token_ids['attention_mask'])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**token_ids)\n",
    "    logits = outputs.logits\n",
    "probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple transformer in PyTorch\n",
    "\n",
    "transformer_model = nn.Transformer(d_model=50, nhead=10, num_encoder_layers=12, batch_first=True)\n",
    "# (batch_size, seq_len, embedding_dim)\n",
    "src = torch.rand((32, 10, 50))\n",
    "tgt = torch.rand((32, 10, 50))\n",
    "out = transformer_model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning hugging face transformer for a task using Trainer\n",
    "\n",
    "# sample data\n",
    "X = [\"The cat is white\", \"I like dogs\"] * 10\n",
    "y = [0, 3] * 10\n",
    "\n",
    "# prepare the data\n",
    "class TransDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# define tokenizer, dataset and model\n",
    "max_len = 50\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = TransDataset(texts=X, labels=y, tokenizer=tokenizer, max_len=max_len)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "# train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2, \n",
    "    warmup_steps=30,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# predict\n",
    "encoding_valid = tokenizer(\n",
    "    [\"The cat is white\", \"I like dogs\"],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding_valid)\n",
    "    print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning hugging face transformer for a task using PyTorch\n",
    "\n",
    "# define tokenizer, dataset and model - the same as previously\n",
    "max_len = 50\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = TransDataset(texts=X, labels=y, tokenizer=tokenizer, max_len=max_len)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# define scheduler\n",
    "num_epochs = 7\n",
    "num_training_steps = num_epochs * len(data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=10, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(loss.item(), lr_scheduler._last_lr)   \n",
    "\n",
    "\n",
    "# evaluate \n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "model.eval()\n",
    "for batch in data_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_preds.extend(predictions.detach().cpu().numpy())\n",
    "    all_labels.extend(batch['labels'].detach().cpu().numpy())\n",
    "\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
