{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train1 = torchvision.datasets.ImageFolder('./Training/')\n",
    "dataset_train2 = torchvision.datasets.ImageFolder('./Training/', \n",
    "                transform=torchvision.transforms.Resize((224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train1[0][0].show()\n",
    "#dataset_train2[0][0].show()\n",
    "print(dataset_train2[0][1])\n",
    "#dataset_train2[-1][0].show()\n",
    "print(dataset_train2[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.ImageFolder('./Training/', transform=composed)\n",
    "dataset_valid = torchvision.datasets.ImageFolder('./Validation/', transform=composed)\n",
    "dataset_train[0][0].shape, dataset_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def compute_error(model, data_loader, criterion, c_sum=False):\n",
    "    model.eval()\n",
    "    losses, num_of_el = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y.type(torch.float32))\n",
    "            if not c_sum: loss *= len(y)\n",
    "            losses += loss\n",
    "            num_of_el += len(y)\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "              train_loader: DataLoader,\n",
    "              valid_loader: DataLoader,\n",
    "              num_epochs: int,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion,\n",
    "              verbose: bool = True,\n",
    "              verbose_plot: bool = False\n",
    "              ) -> float:\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.type(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "            _iter += 1\n",
    "\n",
    "        val_loss = compute_error(model, valid_loader, criterion)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "        if verbose_plot:\n",
    "            train_loss = compute_error(model, train_loader, criterion)\n",
    "            train_losses.append(train_loss.detach().cpu())\n",
    "            valid_losses.append(val_loss.detach().cpu())\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    if verbose_plot:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(train_losses, c='b', label='train')\n",
    "        plt.plot(valid_losses, c='r', label='valid')\n",
    "        plt.grid(ls=':')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using torchvision\n",
    "model = torchvision.models.resnet50(weights='ResNet50_Weights.DEFAULT').to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change the last layer to fit our needs\n",
    "num_classes = 1\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    torch.nn.Flatten(0, 1),\n",
    ").to(device)\n",
    "\n",
    "# train the parameters of the last layer\n",
    "optimizer = torch.optim.NAdam(model.fc.parameters(), lr=0.005)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "train_model(model, train_loader, valid_loader, 5, optimizer, criterion, True, True)\n",
    "\n",
    "# prediction\n",
    "#img = PIL.Image.fromarray((np.random.rand(224, 224, 3) * 255).astype(np.uint8))\n",
    "#model(composed(img)[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    preds = outputs > 0\n",
    "    targets = targets.type(torch.bool)\n",
    "    return sum(preds == targets)\n",
    "compute_error(model, valid_loader, accuracy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with self-implemented resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, size_blocks, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_planes = size_blocks[0]\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "\n",
    "        layers = []\n",
    "        for size, num in zip(size_blocks[1:], num_blocks):\n",
    "            stride = 1 if len(layers) == 0 else 2\n",
    "            layers.append(self._make_layer(block, size, num, stride=stride))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.linear = nn.Linear(size_blocks[-1] * block.expansion, num_classes)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out.flatten() # for binary classification\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [16, 16, 32, 64, 128], [2, 2, 2, 2], 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std\n",
    "\n",
    "composed = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((110, 90)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "dataset_train = torchvision.datasets.ImageFolder('./Training/', transform=composed)\n",
    "train_loader = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "\n",
    "mean, std = 0., 0.\n",
    "img_cnt = 0\n",
    "for images, y in train_loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    img_cnt += batch_samples\n",
    "\n",
    "mean /= img_cnt\n",
    "std /= img_cnt\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train3 = torchvision.datasets.ImageFolder('./Training/', \n",
    "                transform=torchvision.transforms.Resize((110, 90)))\n",
    "#dataset_train3[0][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((110, 90)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.ImageFolder('./Training/', transform=composed)\n",
    "dataset_valid = torchvision.datasets.ImageFolder('./Validation/', transform=composed)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = next(iter(train_loader))\n",
    "img = img.type(torch.float32).to(device)\n",
    "print(model(img))\n",
    "del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.0002)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "train_model(model, train_loader, valid_loader, 1, optimizer, criterion, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    preds = outputs > 0\n",
    "    targets = targets.type(torch.bool)\n",
    "    return sum(preds == targets)\n",
    "compute_error(model, valid_loader, accuracy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Grad(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            nn.ReLU(),\n",
    "            model.layers,\n",
    "        )\n",
    "\n",
    "        self.avg_pool = model.avg_pool\n",
    "        self.linear = model.linear\n",
    "\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x.flatten()\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Net_Grad(model).to(device)\n",
    "grad.eval()\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "img, _ = next(iter(train_loader))\n",
    "img = img.type(torch.float32).to(device)\n",
    "\n",
    "preds = netG(img)\n",
    "preds.backward()\n",
    "gradients = netG.get_activations_gradient()\n",
    "pooled_gradient = torch.mean(gradients, dim=[0, 2, 3])\n",
    "activations = netG.get_activations(img).detach()\n",
    "for i in range(128):\n",
    "    activations[:, i, :, :] *= pooled_gradient[i]\n",
    "heatmap = torch.mean(activations, dim=1).squeeze().cpu()\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= torch.max(heatmap)\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invTrans = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Normalize(\n",
    "        mean = [ 0., 0., 0. ],\n",
    "        std = [ 1/std[0], 1/std[1], 1/std[2] ]),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean = [ -mean[0], -mean[1], -mean[2] ],\n",
    "        std = [ 1., 1., 1. ]),\n",
    "])\n",
    "\n",
    "image = PIL.Image.fromarray((invTrans(img) * 255).detach().cpu().numpy().astype(np.uint8)[0, 0, :, :])\n",
    "print(preds) # >0  => man\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_colored = (plt.cm.jet(heatmap) * 255).astype(np.uint8)\n",
    "heatmap_image = PIL.Image.fromarray(heatmap_colored).resize(image.size, PIL.Image.BILINEAR)\n",
    "blended_image = PIL.Image.blend(image.convert('RGBA'), heatmap_image.convert('RGBA'), alpha=0.5)\n",
    "blended_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
