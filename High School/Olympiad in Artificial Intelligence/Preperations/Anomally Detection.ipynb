{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sensor.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['machine_status'].value_counts())\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "targets = dataset['machine_status']\n",
    "\n",
    "le = LabelEncoder()\n",
    "targets_enc = le.fit_transform(targets)\n",
    "targets_enc, le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_d = dataset.drop(['machine_status', 'Unnamed: 0', 'timestamp'], axis=1)\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "dataset_d['month'] = dataset['timestamp'].dt.month\n",
    "dataset_d['day'] = dataset['timestamp'].dt.day\n",
    "dataset_d['hour'] = dataset['timestamp'].dt.hour\n",
    "dataset_d['minute'] = dataset['timestamp'].dt.minute\n",
    "dataset_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "dataset_prepared = pipeline.fit_transform(dataset_d)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset_prepared = torch.tensor(dataset_prepared, dtype=torch.float, device=device)\n",
    "dataset_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = targets.values != 'NORMAL'\n",
    "id_normal = np.argwhere(y == False).flatten()\n",
    "id_anomaly = np.argwhere(y == True).flatten()\n",
    "len(id_normal), len(id_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normal = dataset_prepared[id_normal]\n",
    "data_anomaly = dataset_prepared[id_anomaly]\n",
    "\n",
    "dataset_normal = TensorDataset(data_normal, torch.tensor(y[id_normal]))\n",
    "dataset_anomaly = TensorDataset(data_anomaly, torch.tensor(y[id_anomaly]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "len_valid = 14484\n",
    "dataset_train, dataset_valid = random_split(dataset_normal, [len(dataset_normal) - len_valid, len_valid])\n",
    "dataset_valid += dataset_anomaly\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#data_loader = DataLoader(TensorDataset(dataset_prepared), batch_size=512, shuffle=True)\n",
    "\n",
    "model = Autoencoder(dataset_prepared.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for inputs, y in dataloader_train:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 2 == 0:    \n",
    "        print(f\"{epoch}: loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting anomalies\n",
    "def detect_anomalies(data, threshold):\n",
    "    #data = torch.tensor(data, dtype=torch.float32)\n",
    "    outputs = model(data)\n",
    "    losses = nn.functional.mse_loss(outputs, data, reduction='none').mean(dim=1)\n",
    "    anomalies = losses > threshold\n",
    "    return anomalies\n",
    "\n",
    "dataset_valid_tensor = torch.cat([dataset_valid[i][0][None, :] for i in range(len(dataset_valid))], dim=0)\n",
    "y_valid_tensor = torch.cat([dataset_valid[i][1][None] for i in range(len(dataset_valid))], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "thresholds = np.arange(0.01, 0.5, 0.01)\n",
    "score_acc = []\n",
    "score_prec = []\n",
    "score_rec = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = detect_anomalies(dataset_valid_tensor, threshold).cpu()\n",
    "    score_acc.append(accuracy_score(y_valid_tensor, preds))\n",
    "    score_prec.append(precision_score(y_valid_tensor, preds))\n",
    "    score_rec.append(recall_score(y_valid_tensor, preds))\n",
    "\n",
    "plt.plot(thresholds, score_acc, label='accuracy_score')\n",
    "plt.plot(thresholds, score_prec, label='precision_score')\n",
    "plt.plot(thresholds, score_rec, label='recall_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "preds = detect_anomalies(dataset_valid_tensor, threshold).cpu()\n",
    "\n",
    "print(sum(preds), sum(y_valid_tensor))\n",
    "#print(np.argwhere(preds))\n",
    "#print(np.argwhere(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \", accuracy_score(y_valid_tensor, preds))\n",
    "print(\"precision: \", precision_score(y_valid_tensor, preds))\n",
    "print(\"recall: \", recall_score(y_valid_tensor, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
