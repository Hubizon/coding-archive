{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyyeHTb4hn_B"
      },
      "source": [
        "## Wyzwanie 'Coding Agent'\n",
        "\n",
        "### Cel Zadania:\n",
        "Uczestnicy mają za zadanie zaprojektować i zaimplementować agenta AI, który będzie zdolny do samodzielnego pisania efektywnego kodu rozwiązującego zadane problemy algorytmiczne.\n",
        "\n",
        "### Opis:\n",
        "W ramach warsztatów uczestnicy stworzą agenta opartego o model językowy gpt-3.5, który będzie w stanie interpretować opisy problemów algorytmicznych i automatycznie generować odpowiedni kod w pythonie. Model powinien radzić sobie z różnorodnymi typami problemów, od prostych algorytmów sortowania po bardziej złożone zagadnienia takie jak algorytmy na grafach czy struktury danych.\n",
        "\n",
        "### Wymagania:\n",
        "1. Wybór i uzasadnienie wykorzystanego systemu agenta AI.\n",
        "2. Implementacja agenta, który będzie testowany na zestawie standardowych problemów algorytmicznych.\n",
        "3. Osiągnięcie przynajmniej 70% na benchmarku 'lvl 1'\n",
        "\n",
        "### Ranking\n",
        "Udostępnij swoje najlepsze accuracy dla zbioru testowego w arkuszu - https://docs.google.com/spreadsheets/d/1jGV7RTz4xHvFUc-8msqLn3jhUiTX6QZ8-7rv6n6z07E/edit?usp=sharing\n",
        "\n",
        "Na koniec warsztatu proszę wrzucić swoje notebooki z rozwiązaniami do repo, katalog `rozwiazania`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP5uiCxnNkX7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAmZPt4MNkX8",
        "outputId": "bb2b9102-f967-4ba4-de30-37f4ecd5a0e4"
      },
      "outputs": [],
      "source": [
        "# load the data from gdrive\n",
        "import gdown\n",
        "\n",
        "GDRIVE_DATA = [(\"1pYt9pH0TXhVaOBOG026v441Mv4qKqwZI\", \"utils.py\"),\n",
        "                (\"1fVjQ4WcRun5xaDd8vwBLEB1ZAfTBCcFm\", \"requirements.txt\"),\n",
        "                (\"1OP-NYC619eZdtrQWBkRY3C0tu-FXnzas\",\"problems_lvl_1.jsonl\"),\n",
        "                (\"19_tVo2yD_PgpLfmwRvr13U-82I5ANqp9\",\"problems_lvl_2.jsonl\"),\n",
        "                (\"1BCHOoMAGGqt65fk3XITpQcqp8pyvfnKO\",\"problems_lvl_3.jsonl\")]\n",
        "\n",
        "for file_id, zip_name in GDRIVE_DATA:\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, zip_name, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNM4U61uNkX-",
        "outputId": "08e931b6-f150-47a2-a17e-c046ff02c0f5"
      },
      "outputs": [],
      "source": [
        "# RUN IF YOU ARE USING COLAB\n",
        "# install requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaE8pNkONkX_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import importlib\n",
        "import multiprocessing\n",
        "from utils import load_problems, test, benchmark, Problem, execute_program, count_tokens\n",
        "\n",
        "problems_lvl_1 = load_problems(lvl=1)\n",
        "problems_lvl_2 = load_problems(lvl=2)\n",
        "problems_lvl_3 = load_problems(lvl=3)\n",
        "# jeżeli nie macie doładowanego konta, proszę się zgłosić\n",
        "# Póki co użyjcie tego klucza - '###'\n",
        "client = openai.OpenAI(api_key=\"###\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQmcalMuNkX_",
        "outputId": "216ae85f-012d-4734-bce1-2fa5f6af83cb"
      },
      "outputs": [],
      "source": [
        "print(problems_lvl_3[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkBjZk2Nih7n"
      },
      "source": [
        "## Przykład zadania\n",
        "\n",
        "Twój algorytm może korzystać jedynie z atrybutów problemu `statement`, `function_signature` i `example_test`. Atrybut `tests` będzie używany jedynie podczas testowania. Aby uzyskać więcej informacji zapoznaj się z klasą `Problem` oraz funkcją `test` z pliku `utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgBAnBUNiuud"
      },
      "outputs": [],
      "source": [
        "def baseline(problem):\n",
        "    prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:'\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Solve given problem in python. Return just the code of function.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mytUyp-PNkYB",
        "outputId": "fd6ffa99-765a-4f28-dc2e-20d8648806a0"
      },
      "outputs": [],
      "source": [
        "result = test(baseline, problems_lvl_1[0], verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-Ey_TUNkYC"
      },
      "source": [
        "## Guideline\n",
        "\n",
        "Zbiór `lvl_1` ma niecałe 1000 zadań. Nie mamy czasu, aby testować rozwiązań na nich wszystkich. Przyjmijmy, że na potrzeby rozwoju agentów (`train set`) będziemy używać losowo wybranych 50 zadań. Aby to osiągnąć należy ustawić wartość `seed` w funkcji benchmark na dowolną wartość całkowitą. Nasze rozwiązania będziemy testować na defaultowej wartości `seed=None`, która bierze pierwsze n zadań ze zbioru."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG7EMrdOilPj"
      },
      "source": [
        "# Twoje Rozwiązania - Lvl 1\n",
        "\n",
        "Zacznij od uzupełnienia definicji agentów poniżej. Następnie przetestuj swoje rozwiązania funkcją `benchmark`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAOe4Fq0NkYC"
      },
      "source": [
        "[Large Language Models are Zero-Shot Reasoners - arXiv](https://arxiv.org/pdf/2205.11916)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7MDH4YiSWCYO"
      },
      "outputs": [],
      "source": [
        "def get_prompt(problem):\n",
        "  prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:' End with '{problem.example_test}'\"\n",
        "  return prompt\n",
        "\n",
        "prompt1 = get_prompt(problems_lvl_1[0])\n",
        "\n",
        "answer1 = \"\"\"```python\n",
        "def min_cost(cost, m, n):\n",
        "    dp = [[0 for _ in range(n+1)] for _ in range(m+1)]\n",
        "\n",
        "    dp[0][0] = cost[0][0]\n",
        "\n",
        "    for i in range(1, m+1):\n",
        "        dp[i][0] = dp[i-1][0] + cost[i][0]\n",
        "\n",
        "    for j in range(1, n+1):\n",
        "        dp[0][j] = dp[0][j-1] + cost[0][j]\n",
        "\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            dp[i][j] = cost[i][j] + min(dp[i-1][j], dp[i][j-1])\n",
        "\n",
        "    return dp[m][n]\n",
        "```\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YSLtJfN5WpTC"
      },
      "outputs": [],
      "source": [
        "def get_prompt(problem):\n",
        "  prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:' End with '{problem.example_test}' to test the corectness\"\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGP5nMYWVPa",
        "outputId": "c465d3e5-fdc4-43c5-8b90-4c714c95934b"
      },
      "outputs": [],
      "source": [
        "def baseline(problem):\n",
        "    prompt = get_prompt(problem)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Solve given problem in python. Return just the code of function.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "test(baseline, problems_lvl_1[1], verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrKVEtpHNkYD"
      },
      "source": [
        "## Zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVEr2LKcvVsQ"
      },
      "outputs": [],
      "source": [
        "def coding_agent_zero_shot(problem):\n",
        "    prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:'\"\n",
        "    #prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:' The example test is: {problem.example_test} make sure that it will work on this test\"\n",
        "    #prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:'. Make sure this assert doesn't return an error: {problem.example_test}\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Solve given problem in python. Return just the code of function.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub4sMVRRNkYD",
        "outputId": "d686d8f2-2a30-460d-c653-463f4ecc9258"
      },
      "outputs": [],
      "source": [
        "result = test(coding_agent_zero_shot, problems_lvl_1[0], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyjWjYFANkYD",
        "outputId": "2a9c40ac-f357-4f06-ab49-f6645faff642"
      },
      "outputs": [],
      "source": [
        "benchmark(coding_agent_zero_shot, problems_lvl_1, n_samples=50, verbose=False, seed=44)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blGwTQ5ji0DS"
      },
      "source": [
        "# Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxn67vvQNkYD"
      },
      "outputs": [],
      "source": [
        "def coding_agent_few_shot(problem):\n",
        "    prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:'\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Solve given problem in python. Return just the code of function.\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYH5wapvNkYE"
      },
      "outputs": [],
      "source": [
        "result = test(coding_agent_few_shot, problems_lvl_3[1], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQOLGtmqNkYE"
      },
      "outputs": [],
      "source": [
        "benchmark(coding_agent_few_shot, problems_lvl_3, n_samples=5, verbose=False, seed=44)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFCrq4UfNkYE"
      },
      "source": [
        "## Chain-of-thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loZFcRY9NkYE"
      },
      "outputs": [],
      "source": [
        "def coding_agent_chain_of_thoughts(problem):\n",
        "    prompt = f\"{problem.statement} Start with 'def {problem.function_signature}:'\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"**write instruction in here**\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": \"**Task**\"},\n",
        "            {\"role\": \"assistant\", \"content\":\"**Solution**\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_o8dLDNNkYE"
      },
      "outputs": [],
      "source": [
        "result = test(coding_agent_chain_of_thoughts, problems_lvl_1[0], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5GnXckVNkYE"
      },
      "outputs": [],
      "source": [
        "benchmark(coding_agent_chain_of_thoughts, problems_lvl_1, n_samples=50, verbose=False, seed=44)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4L-MOBaNkYE"
      },
      "source": [
        "# Twoje Rozwiązania - Developer & Tester"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOeeg1XaNkYE"
      },
      "source": [
        "[Teaching Large Language Models to Self-Debug - arXiv](https://arxiv.org/pdf/2304.05128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQnZ0UpNkYE"
      },
      "source": [
        "Poniżej zaimplementowany jest właśnie ten schemat. Mamy dwóch agentów - dewelopera i testera. Pierwszy stara się rozwiązać problem. Drugi wykonuje kod i komentuje rozwiązanie. Rozwiązanie zostaje zwrócone, gdy przejdzie przykładowe testy lub limit iteracji zostanie przekroczony."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQySh6alNkYF"
      },
      "source": [
        "## Iteracyjne ulepszanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgxhFuosNkYF"
      },
      "outputs": [],
      "source": [
        "class SelfDebuggingAgent_continuous_development:\n",
        "    def __init__(self):\n",
        "        self.context_tokens_limit = 16572\n",
        "        self.iterations = 3\n",
        "        self.tester_system_message = \"**WRITE YOUR PROMPT IN HERE**\"\n",
        "        self.developer_system_message = \"**WRITE YOUR PROMPT IN HERE**\"\n",
        "        self.chat_history_developer_perspective = []\n",
        "        self.chat_history_tester_perspective = []\n",
        "        self.problem = None\n",
        "        self.solution = None\n",
        "\n",
        "    def Tester(self):\n",
        "        # it executes the proposed solution, analyse the output and return the feedback\n",
        "        execution_logs, is_correct = self.execute_code(self.solution, self.problem.example_test)\n",
        "        self.chat_history_tester_perspective[-1][\"content\"] += f\"Execution logs:\\n{execution_logs}\"\n",
        "        feedback = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.chat_history_tester_perspective\n",
        "        )\n",
        "\n",
        "        self.chat_history_developer_perspective.append({\"role\": \"user\", \"content\": feedback.choices[0].message.content})\n",
        "        self.chat_history_tester_perspective.append({\"role\": \"assistant\", \"content\": feedback.choices[0].message.content})\n",
        "        self.chat_history_tester_perspective.append({\"role\": \"user\", \"content\": \"\"})\n",
        "\n",
        "        return is_correct\n",
        "\n",
        "    def Developer(self):\n",
        "        # it generates a solution for the problem\n",
        "        code = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.chat_history_developer_perspective\n",
        "        )\n",
        "        self.solution = code.choices[0].message.content\n",
        "        self.chat_history_developer_perspective.append({\"role\": \"assistant\", \"content\": self.solution})\n",
        "        self.chat_history_tester_perspective[-1][\"content\"] += f\"Proposed code:\\n {self.solution}\\n\\n\"\n",
        "\n",
        "\n",
        "    def execute_code(self, code, tests):\n",
        "        if type(tests) == str:\n",
        "            program = code + \"\\n\" + tests\n",
        "        elif type(tests) == list:\n",
        "            program = \"\\n\".join([code] + tests)\n",
        "\n",
        "        q = multiprocessing.Queue()\n",
        "        process = multiprocessing.Process(target=execute_program, args=(program, q))\n",
        "        process.start()\n",
        "        process.join(10.0)\n",
        "\n",
        "        if process.is_alive():\n",
        "            process.terminate()\n",
        "            process.join()\n",
        "            is_correct, error = False, \"Timeout\"\n",
        "        elif q.empty():\n",
        "            is_correct, error = False, \"Unknown\"\n",
        "        else:\n",
        "            is_correct, exception = q.get()\n",
        "            if exception is None:\n",
        "                error = None\n",
        "            else:\n",
        "                error = \"Tests failed\" if isinstance(exception, AssertionError) else str(exception)\n",
        "\n",
        "        execution_logs = f\"Passed all tests? {is_correct}\" + f'\\nError: {error}' if error else \"\"\n",
        "        return execution_logs, is_correct\n",
        "\n",
        "    def count_chat_tokens(self,chat_history):\n",
        "        # return sum([count_tokens(turn[\"content\"]) for turn in chat_history])\n",
        "        return sum([len(turn[\"content\"].split())*4/3.5 for turn in chat_history])\n",
        "\n",
        "    def initialize_chat(self,problem):\n",
        "        self.chat_history_developer_perspective = [{\"role\": \"system\", \"content\": self.developer_system_message}]\n",
        "        self.chat_history_tester_perspective = [{\"role\": \"system\", \"content\": self.tester_system_message}]\n",
        "\n",
        "        # tester initialization per problem\n",
        "        self.problem = problem\n",
        "        self.chat_history_tester_perspective.append({\"role\": \"user\", \"content\": self.problem.statement + \" Start with 'def \" + self.problem.function_signature + \":'\\n\\n\"})\n",
        "\n",
        "        # developer initialization per problem\n",
        "        prompt = f\"{self.problem.statement} Start with 'def {self.problem.function_signature}:'\"\n",
        "        self.chat_history_developer_perspective.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    def __call__(self, problem: Problem):\n",
        "        self.initialize_chat(problem)\n",
        "        self.Developer()\n",
        "        for _ in range(self.iterations):\n",
        "            if self.count_chat_tokens(self.chat_history_tester_perspective) > self.context_tokens_limit:\n",
        "                break\n",
        "            is_correct = self.Tester()\n",
        "            if is_correct or self.count_chat_tokens(self.chat_history_developer_perspective) > self.context_tokens_limit:\n",
        "                break\n",
        "            self.Developer()\n",
        "        return self.solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO-myPyMNkYF"
      },
      "outputs": [],
      "source": [
        "benchmark(SelfDebuggingAgent_continuous_development(), problems_lvl_1, n_samples=50, verbose=False, seed=44)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5UubaEdNkYF"
      },
      "source": [
        "## Zoptymalizowana wersja\n",
        "\n",
        "W tym przypadku model nie zapamiętuje całej historii konwersacji. Deweloper dostaje jedynie treść, poprzednie rozwiązanie i feedback of testera. Tester dostaje jedynie treść zadania oraz najnowsze wygenerowane rozwiązanie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmY58Fp1NkYF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SelfDebuggingAgent:\n",
        "    def __init__(self):\n",
        "        self.context_tokens_limit = 16572\n",
        "        self.iterations = 5\n",
        "        self.tester_system_message = \"**WRITE YOUR PROMPT IN HERE**\"\n",
        "        self.developer_system_message = \"**WRITE YOUR PROMPT IN HERE**\"\n",
        "        self.chat_history_developer_perspective = []\n",
        "        self.chat_history_tester_perspective = []\n",
        "        self.problem = None\n",
        "        self.solution = None\n",
        "\n",
        "    def Tester(self):\n",
        "        # it executes the proposed solution, analyse the output and return the feedback\n",
        "        self.chat_history_tester_perspective[1] = {\"role\": \"user\", \"content\": self.problem.statement + \" Start with 'def \" + self.problem.function_signature + \":'\\n\\n\"}\n",
        "        self.chat_history_tester_perspective[1][\"content\"] += f\"Proposed code:\\n {self.solution}\\n\\n\"\n",
        "        execution_logs, is_correct = self.execute_code(self.solution, self.problem.example_test)\n",
        "        self.chat_history_tester_perspective[1][\"content\"] += f\"Execution logs:\\n{execution_logs}\"\n",
        "        feedback = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.chat_history_tester_perspective\n",
        "        )\n",
        "\n",
        "        self.chat_history_developer_perspective[3] = {\"role\": \"user\", \"content\": feedback.choices[0].message.content}\n",
        "\n",
        "        return is_correct\n",
        "\n",
        "    def Developer(self):\n",
        "        # it generates a solution for the problem\n",
        "        code = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.chat_history_developer_perspective\n",
        "        )\n",
        "\n",
        "        self.solution = code.choices[0].message.content\n",
        "\n",
        "\n",
        "    def execute_code(self, code, tests):\n",
        "        if type(tests) == str:\n",
        "            program = code + \"\\n\" + tests\n",
        "        elif type(tests) == list:\n",
        "            program = \"\\n\".join([code] + tests)\n",
        "\n",
        "        q = multiprocessing.Queue()\n",
        "        process = multiprocessing.Process(target=execute_program, args=(program, q))\n",
        "        process.start()\n",
        "        process.join(10.0)\n",
        "\n",
        "        if process.is_alive():\n",
        "            process.terminate()\n",
        "            process.join()\n",
        "            is_correct, error = False, \"Timeout\"\n",
        "        elif q.empty():\n",
        "            is_correct, error = False, \"Unknown\"\n",
        "        else:\n",
        "            is_correct, exception = q.get()\n",
        "            if exception is None:\n",
        "                error = None\n",
        "            else:\n",
        "                error = \"Tests failed\" if isinstance(exception, AssertionError) else str(exception)\n",
        "\n",
        "        execution_logs = f\"Passed all tests? {is_correct}\" + f'\\nError: {error}' if error else \"\"\n",
        "        return execution_logs, is_correct\n",
        "\n",
        "    def initialize_chat(self,problem):\n",
        "        # tester initialization per problem\n",
        "        self.chat_history_tester_perspective = [{\"role\": \"system\", \"content\": self.tester_system_message}]\n",
        "        self.problem = problem\n",
        "        self.chat_history_tester_perspective.append({\"role\": \"user\", \"content\": self.problem.statement + \" Start with 'def \" + self.problem.function_signature + \":'\\n\\n\"})\n",
        "\n",
        "        # developer initialization per problem\n",
        "        self.chat_history_developer_perspective = [{\"role\": \"system\", \"content\": self.developer_system_message}]\n",
        "        prompt = f\"{self.problem.statement} Start with 'def {self.problem.function_signature}:'\"\n",
        "        self.chat_history_developer_perspective.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    def __call__(self, problem: Problem):\n",
        "        self.initialize_chat(problem)\n",
        "        self.Developer()\n",
        "        for i in range(self.iterations):\n",
        "            if i == 0:\n",
        "                self.chat_history_developer_perspective.append({\"role\":\"assistant\",\"content\":self.solution})\n",
        "                self.chat_history_developer_perspective.append({\"role\":\"user\",\"content\":\"\"})\n",
        "            else:\n",
        "                self.chat_history_developer_perspective[2] = {\"role\":\"assistant\",\"content\":self.solution}\n",
        "\n",
        "            is_correct = self.Tester()\n",
        "            if is_correct:\n",
        "                break\n",
        "            self.Developer()\n",
        "        return self.solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6qXMfHUNkYF"
      },
      "outputs": [],
      "source": [
        "benchmark(SelfDebuggingAgent(), problems_lvl_1, n_samples=50, verbose=False, seed=44)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JehOFFmNkYG"
      },
      "source": [
        "# Twoje Rozwiązania - Lvl 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnofVx7ONkYG"
      },
      "source": [
        "W tej części skupiamy się na jakości agenta, nie na optymalności czasowej. Użyj modelu `gpt-4o`, aby rozwiązać kilka zadań z Olimpiady Informatycznej (gimnazjalistów). Nie ma wymogów formalnych dotyczących rozwiązania. Możecie korzystać z dowolnych frameworków i gotowych rozwiązań jakie znajdziecie w internecie.\n",
        "\n",
        "Dobrym początkiem będzie prześledzenie jak inni radzą sobie z tego typu zadaniem - https://paperswithcode.com/sota/code-generation-on-mbpp\n",
        "\n",
        "Jako krok pośredni możesz przetestować swój algorytm na zbiorze `lvl_2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH_0nM0mNkYG"
      },
      "outputs": [],
      "source": [
        "def the_coding_agent(problem: Problem):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9BchrT1NkYG"
      },
      "outputs": [],
      "source": [
        "benchmark(the_coding_agent(), problems_lvl_3, n_samples=5, verbose=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
