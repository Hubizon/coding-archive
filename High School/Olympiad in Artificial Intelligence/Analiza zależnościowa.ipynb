{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza zależnościowa\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "Język, którym posługujemy się na co dzień, funkcjonuje na zasadzie kompozycyjności. Oznacza to, że znaczenie złożonych wyrażeń językowych można wywnioskować z ich części składowych i z relacji między nimi. Ta właściwość pozwala użytkownikom języka na daleko idącą kreatywność w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Sposób w jaki słowa w zdaniu są ze sobą związane, tworzy strukturę ukorzenionego drzewa. Problemem, który rozważamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zdań w języku polskim. Problem nosi nazwę analizy składniowej zdań, a konkretnie dokonywać będziemy analizy zależnościowej. \n",
    "\n",
    "Analiza składniowa jest w ogólności trudna. Na przykład, mimo że zdania `(1) Maria do jutra jest zajęta.` oraz `(2) Droga do domu jest zajęta.` zawierają kolejno te same części mowy, w dodatku o dokładnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajęta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzędnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni użytkownicy języka mogą zinterpretować strukturę zdania na dwa różne sposoby: zdanie `Zauważyłem dziś samochód Adama, którego dawno nie widziałem.` może być interpretowane na dwa sposoby w zależności od tego, do czego odnosi się \"którego\": czy do \"samochodu Adama\", czy może do \"Adama\".\n",
    "\n",
    "Istnieje wiele różnych algorytmów rozwiązujących problem analizy zależnościowej. Klasyczne metody przetwarzają zdanie słowo po słowie, od lewej do prawej i wstawiają krawędzie w oparciu albo o pewien ustalony zbiór reguł lub o algorytm uczenia maszynowego. W tym zadaniu użyjemy innej metody. Twoim zadaniem będzie przewidzenie drzewa zależnościowego w oparciu o wektory słów otrzymane modelem HerBERT.\n",
    "\n",
    "HerBERT to polska wersja BERT, który jest modelem językowym i działa następująco:\n",
    "1. BERT posiada moduł nazywany tokenizatorem (ang. tokenizer), który dzieli zdanie na pewne podsłowa. Na przykład zdanie `Dostaję klucz i biegnę do swojego pokoju.` dzieli na `'Dosta', 'ję', 'klucz', 'i', 'bieg', 'nę', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposażony w słownik, który podsłowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy mało zrozumiałe dla człowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n",
    "1. Następnie BERT posiada słownik, który zamienia te liczby na wektory o długości 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n",
    "1. BERT posiada 12 warstw, z których każda bierze wynik poprzedniej i wykonuje na niej pewną transformację. Szczegóły nie są istotne w tym zadaniu! Ważne jest natomiast to, że cały model jest uczony automatycznie, przy użyciu dużych korpusów tekstu. Zinterpretowanie działania każdej warstwy jest niemożliwe! Natomiast być może w skomplikowanym algorytmie, którego nauczył się BERT różne warstwy pełnią różne role.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Twoim zadaniem będzie automatyczna analiza składniowa zdań w języku polskim. Pominiemy dokładne objaśnienie sposobu konstruowania takich drzew, możesz samemu popatrzeć na przykłady! Dostaniesz zbiór danych treningowych zawierający 1000 przykładów rozkładów zdań. W pliku `train.conll` znajdują się poetykietowane zdania, na przykład:\n",
    "\n",
    "| # | Word      | - | - | - | - | Head | - | - | - |\n",
    "|---|-----------|---|---|---|---|--------|---|---|---|\n",
    "| 1 | Wyobraź   | _ | _ | _ | _ | 0      | _ | _ | _ |\n",
    "| 2 | sobie     | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 3 | człowieka | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 4 | znajdującego | _ | _ | _ | _ | 3    | _ | _ | _ |\n",
    "| 5 | się       | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 6 | na        | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 7 | ogromnej  | _ | _ | _ | _ | 8      | _ | _ | _ |\n",
    "| 8 | górze     | _ | _ | _ | _ | 6      | _ | _ | _ |\n",
    "| 9 | .         | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "\n",
    "Co jest sposobem na zakodowanie następującego drzewa składniowego zdania złożonego:\n",
    "```\n",
    "      Wyobraź                          \n",
    "   ______|_____________                 \n",
    "  |      |         człowieka           \n",
    "  |      |             |                \n",
    "  |      |        znajdującego         \n",
    "  |      |      _______|__________      \n",
    "  |      |     |                  na   \n",
    "  |      |     |                  |     \n",
    "  |      |     |                górze  \n",
    "  |      |     |                  |     \n",
    "sobie    .    się              ogromnej\n",
    "```\n",
    "Dostarczamy Ci funkcję w Pythonie służącą do wczytania przykładów z tego pliku i na ich wizualizację. Twoje rozwiązanie powinno:\n",
    "1. Dzielić zdanie na podsłowa.\n",
    "1. Dla każdego podsłowa przypisywać wektor. Należy użyć tutaj finalnych lub pośrednich wektorów wyliczonych przez model HerBERT.\n",
    "1. Agregować wektory podsłów tak aby otrzymać wektory słów.\n",
    "1. Zaimplementować i wyuczyć prosty model przewidujący odległości w drzewie i głębokości w drzewie poszczególnych słów w zdaniu.\n",
    "1. Użyć modeli odległości i głębokości do skonstruowania drzewa składniowego.\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n",
    "- Ewaluacja twojego rozwiązania (bez treningu) na 200 przykładach testowych powinna trwać nie dłużej niż 5 minut na Google Colab bez GPU.\n",
    "- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystać z innych uprzednio wytrenowanych modeli oraz ze zbiorów danych innych niż dostarczony. \n",
    "- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`. \n",
    "\n",
    "## Uwagi i wskazówki\n",
    "- Liczne wskazówki znajdują się we wzorcach funkcji, które powinieneś zaimplementować.\n",
    "\n",
    "## Pliki zgłoszeniowe\n",
    "Rozwiązanie zadania stanowi plik archiwum zip zawierające:\n",
    "1. Ten notebook\n",
    "2. Plik z wagami modelu odległości: `distance_model.pth`\n",
    "3. Plik z wagami modelu głębokości: `depth_model.pth`\n",
    "\n",
    "Uruchomienie całego notebooka z flagą `FINAL_EVALUATION_MODE` ustawioną na `False` powinno w maksymalnie 10 minut skutkować utworzeniem obu plików z wagami.\n",
    "\n",
    "## Ewaluacja\n",
    "Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a następnie zostanie uruchomiony cały notebook.\n",
    "Zaimplementowana przez Ciebie funkcja `parse_sentence`, której wzorzec znajdziesz na końcu tego notatnika, zostanie oceniona na 200 przykładach testowych.\n",
    "Ewaluacja będzie podobna do tej zaimplementowanej w funkcji `evaluate_model`. \n",
    "Pamiętaj jednak, że ostateczna funkcja do ewaluacji sprawdzała będzie dodatkowo, czy zwracane przez twoją funkcję `parse_sentence` drzewa są poprawne!\n",
    "\n",
    "Ewaluacja nie może zajmować więcej niż 3 minuty. Możesz uruchomić walidację swojego rozwiązania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonać się czy nie przekraczasz czasu.\n",
    "Za pomocą skryptu `validation_script.py` będziesz mógł upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających:\n",
    "\n",
    "```\n",
    "python3 validation_script.py --train\n",
    "python3 validation_script.py\n",
    "```\n",
    "\n",
    "Podczas sprawdzania zadania, użyjemy dwóch metryk: UUAS oraz root placement.\n",
    "1. Root placemenet oznacza ułamek przykładów na których poprawnie wskażesz korzeń drzewa składniowego,\n",
    "2. UUAS dla konkretnego zdania to ułamek poprawnie umieszczonych krawędzi. UUAS dla zbioru to średnia wyników dla poszczególnych zdań.\n",
    "\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 2 punkty. Twój wynik za to zadanie zostanie wyliczony za pomocą funkcji:\n",
    "```Python\n",
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "    return (scale(root_placement) + scale(uuas))\n",
    "```\n",
    "Innymi słowy, twój wynik jest sumą wyników za root placement i UUAS. Wynik za daną metrykę jest 0 jeśli wartość danej metryki jest poniżej 0.5 i 1 jeśli jest powyżej 0.85. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością metryki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_EVALUATION_MODE = False  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n",
    "DEPTH_MODEL_PATH = 'depth_model.pth'  # Nie zmieniaj!\n",
    "DISTANCE_MODEL_PATH = 'distance_model.pth'  # Nie zmieniaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModel, AutoTokenizer, PreTrainedModel,\n",
    "                          PreTrainedTokenizer)\n",
    "from utils import (ListDataset, ParsedSentence, Sentence, merge_subword_tokens,\n",
    "                   read_conll, uuas_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_conll('train.conll')  # 1000 zdań\n",
    "val_sentences = read_conll('valid.conll')  # 200 zdań\n",
    "\n",
    "train_sentences[6].pretty_print()  # wyświetl drzewo jednego zdania\n",
    "print(train_sentences[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twoje rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubert Jastrzębski - V LO Kraków\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(sentence: ParsedSentence):\n",
    "    \"\"\"Znajdź odległości między każdą parą słów w zdaniu.\n",
    "    Zwraca macierz numpy o wymiarach (len(sentence), len(sentence)).\"\"\"\n",
    "\n",
    "    ls = len(sentence)\n",
    "    distances = np.zeros((ls, ls))\n",
    "\n",
    "    def DFS(u, v, p, d):\n",
    "        distances[u][v] = d\n",
    "        G = deepcopy(sentence.node_to_children[v])\n",
    "        if sentence.heads[v] != 0:\n",
    "            G += [sentence.heads[v] - 1]\n",
    "        for nv in G:\n",
    "            if nv != p:\n",
    "                DFS(u, nv, v, d + 1)\n",
    "\n",
    "    for u in range(ls):\n",
    "        DFS(u, u, u, 0)\n",
    "\n",
    "    return distances\n",
    "\n",
    "print(get_distances(train_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(\n",
    "    sentences_s: List[str],\n",
    "    tokenizer: PreTrainedTokenizer, \n",
    "    model: PreTrainedModel,\n",
    "    progress_bar: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Funkcja zwraca embeddingi podsłów dla listy zdań.\n",
    "\n",
    "    Argumenty:\n",
    "        sentences_s: Lista zdań. Każde zdanie jest reprezentowane jako string.\n",
    "        tokenizer: Tokenizator HERBERT\n",
    "        model: Model HERBERT\n",
    "        progress_bar: Czy wyświetlać pasek postępu.\n",
    "\n",
    "    Zwraca:\n",
    "        tokens: Lista, która dla każdego zdania zawiera listę tokenów podsłów tego zdania.\n",
    "        embeddings: Lista, która dla każdego zdania zawiera listę tensorów o wymiarach \n",
    "            (seq_len, emb_dim). Zauważ, że seq_len może być różne dla różnych zdań.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wskazówki:\n",
    "    #  1. Możesz użyć funkcji:\n",
    "    #   encoded = tokenizer.batch_encode_plus(...)\n",
    "    #   with torch.no_grad():\n",
    "    #     model(**encoded, output_hidden_states=True)\n",
    "    #  2. Aby przyspieszyć obliczenia, pamiętaj o zgrupowaniu (batching) zdań, przed podaniem ich do modelu.\n",
    "    #  3. Pamiętaj, że każde zdanie może mieć inną długość, więc żeby wypełnić dodatkowe miejsce w zwracanym\n",
    "    #   tensorze, HERBERT zastosuje padding. Pamiętaj o usunięciu paddingu z wyników.\n",
    "    #  4. Tokenizator i model używa specjalnych tokenów (np. początku i końca zdania), które również powinny \n",
    "    #   zostać usunięte.\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    tokens, embeddings = [], []\n",
    "    batches = [sentences_s[i:i+batch_size] for i in range(0, len(sentences_s), batch_size)]\n",
    "\n",
    "    for batch in (tqdm(batches) if progress_bar else batches):\n",
    "        batch_encoded = tokenizer.batch_encode_plus(batch, padding=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            batch_embeds = model(**batch_encoded, output_hidden_states=True)\n",
    "            for encoded, embeds in zip(batch_encoded['input_ids'], batch_embeds['last_hidden_state']):\n",
    "                mask = encoded > 4 # 2 czy 4?\n",
    "                tokens.append(encoded[mask])\n",
    "                embeddings.append(embeds[mask])\n",
    "\n",
    "    return tokens, embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(sentences: List[Sentence], tokenizer, model):\n",
    "    \"\"\"Funkcja zwraca embeddingi słów dla listy zdań, używając modelu i tokenizatora.\"\"\"\n",
    "\n",
    "    # Wskazówki:\n",
    "    #  1. Użyj funkcji get_bert_embeddings do uzyskania embeddingów podsłów.\n",
    "    #  2. Użyj funkcji merge_subword_tokens do uzyskania embeddingów słów.\n",
    "\n",
    "    sentences_s = [str(sentence) for sentence in sentences]\n",
    "    bert_tokens, bert_embeddings = get_bert_embeddings(sentences_s, tokenizer, model, False)\n",
    "\n",
    "    embeddings = []\n",
    "    for sentence, bert_token, bert_embedding in zip(sentences, bert_tokens, bert_embeddings):\n",
    "        embedding = merge_subword_tokens(sentence.words, bert_token, bert_embedding, tokenizer, agg_fn)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_fn(sub_embeddings):\n",
    "    embeddings = torch.mean(sub_embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(sentences: List[ParsedSentence], tokenizer, model):\n",
    "    embeddings = get_word_embeddings(sentences, tokenizer, model)\n",
    "    distances = [get_distances(sent) for sent in sentences]\n",
    "    depths = [dist[sent.root][..., None] for dist, sent in zip(distances, sentences)]\n",
    "    dataset_dist = ListDataset(list(zip(embeddings, distances, sentences)))\n",
    "    dataset_depth = ListDataset(list(zip(embeddings, depths, sentences)))\n",
    "    return dataset_dist, dataset_depth\n",
    "\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    trainset_dist, trainset_depth =  get_datasets(train_sentences, tokenizer, model)\n",
    "    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arrays(sequence, pad_with=np.inf):\n",
    "    \"\"\"\n",
    "    Zakłada, że sequence zawiera tablice (ndarrays) o takiej samej liczbie wymiarów.\n",
    "    Zwraca tensor, zawierający dane dopełnione do tych samych wymiarów wartością pad_with, \n",
    "    gdzie indeks sekwencji odpowiada pierwszemu wymiarowi.\n",
    "    \"\"\"\n",
    "\n",
    "    shapes = np.array([list(seq.shape) for seq in sequence])\n",
    "    max_lens = list(shapes.max(axis=0))\n",
    "    padded = [np.pad(\n",
    "                seq, \n",
    "                tuple((0, max_lens[i] - seq.shape[i]) for i in range(seq.ndim)), \n",
    "                'constant', \n",
    "                constant_values=pad_with\n",
    "            ) for seq in sequence]\n",
    "    return torch.tensor(padded)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeddings, targets, sentences = zip(*batch)\n",
    "    padded_embeddings = pad_arrays(embeddings, pad_with=0)\n",
    "    padded_targets = pad_arrays(targets, pad_with=np.inf)\n",
    "    mask = padded_targets != torch.inf\n",
    "    return padded_embeddings, padded_targets, mask, sentences\n",
    "\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    dist_trainloader = DataLoader(trainset_dist, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    dist_valloader = DataLoader(valset_dist, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    depth_trainloader = DataLoader(trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    depth_valloader = DataLoader(valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# dist_trainloader i dist_valloader zwracają krotki (embeddings, distances, masks, sentences)\n",
    "# depths_trainloader i depths_valloader zwracają krotki (embeddings, depths, masks, sentences)  \n",
    "# embeddings.shape: (batch_size, max_seq_len, emb_dim)\n",
    "# distances.shape: (batch_size, max_seq_len, max_seq_len)\n",
    "# depths.shape: (batch_size, max_seq_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 45\n",
    "\n",
    "def compute_error(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = 0\n",
    "    num_of_el = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, masks, sentences in dataloader:\n",
    "            input_lengths = torch.sum(masks, axis=1)[:, :1].reshape(-1)\n",
    "            outputs = model(inputs, input_lengths)\n",
    "            num_of_el += 1\n",
    "            losses += loss_fn(outputs.float(), targets.float(), masks)\n",
    "\n",
    "    return losses / num_of_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(768, 128, batch_first=True, num_layers=3, bidirectional=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, max_input_length)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        packed_inputs = pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, _ = self.rnn(packed_inputs)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        outputs = self.layers(outputs)\n",
    "        outputs = outputs[:, :, :len(packed_inputs.batch_sizes)]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DepthModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(768, 64, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        packed_inputs = pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, _ = self.rnn(packed_inputs)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask):\n",
    "    # mask, target, output to tensory o tym samym kształcie\n",
    "    # mask zawiera 1 tam, gdzie target zawiera dane, a 0 tam, gdzie jest padding\n",
    "\n",
    "    output_masked = output[mask]\n",
    "    target_masked = target[mask]\n",
    "\n",
    "    loss = F.mse_loss(output_masked, target_masked)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, valloader, epochs, lr, verbose=False):\n",
    "    \"\"\"Pętla ucząca twoich modeli.\"\"\"\n",
    "    \n",
    "    optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets, masks, sentences in dataloader:\n",
    "            input_lengths = torch.sum(masks, axis=1)[:, :1].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, input_lengths)\n",
    "            loss = loss_fn(outputs.float(), targets.float(), masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "\n",
    "            _iter +=1\n",
    "\n",
    "        val_loss = compute_error(model, valloader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            #clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\")\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "\n",
    "# W czasie ewaluacji, modele nie powinny być ponownie trenowane.\n",
    "if not FINAL_EVALUATION_MODE: \n",
    "    print(\"Training depth model\")\n",
    "    depth_model = DepthModel()\n",
    "    \n",
    "    lr_depth = 0.008\n",
    "    epochs_depth = 5\n",
    "\n",
    "    train_model(depth_model, depth_trainloader, depth_valloader, lr=lr_depth, epochs=epochs_depth)  \n",
    "    # zapisz wagi modelu do pliku\n",
    "    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n",
    "\n",
    "    print(\"Training distance model\")\n",
    "    distance_model = DistanceModel()\n",
    "    \n",
    "    lr_dist = 0.00325\n",
    "    epochs_dist = 35\n",
    "\n",
    "    train_model(distance_model, dist_trainloader, dist_valloader, lr=lr_dist, epochs=epochs_dist)\n",
    "    # zapisz wagi modelu do pliku\n",
    "    torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self, numOfElements):\n",
    "        self.parent = self.makeSet(numOfElements)\n",
    "        self.size = [1] * numOfElements\n",
    "    \n",
    "    def makeSet(self, numOfElements):\n",
    "        return [x for x in range(numOfElements)]\n",
    "\n",
    "    def find(self, node):\n",
    "        while node != self.parent[node]:\n",
    "            self.parent[node] = self.parent[self.parent[node]]\n",
    "            node = self.parent[node]\n",
    "        return node\n",
    "    \n",
    "    def union(self, node1, node2):\n",
    "        root1 = self.find(node1)\n",
    "        root2 = self.find(node2)\n",
    "\n",
    "        if root1 == root2:\n",
    "            return\n",
    "\n",
    "        if self.size[root1] > self.size[root2]:\n",
    "            self.parent[root2] = root1\n",
    "            self.size[root1] += 1\n",
    "        else:\n",
    "            self.parent[root1] = root2\n",
    "            self.size[root2] += 1\n",
    "\n",
    "    def is_connected(self, node1, node2):\n",
    "        root1 = self.find(node1)\n",
    "        root2 = self.find(node2)\n",
    "\n",
    "        return root1 == root2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sent: Sentence, distance_model, depth_model, tokenizer, model) -> ParsedSentence:\n",
    "    \"\"\"Zbuduj drzewo składniowe dla pojedynczego zdania.\n",
    "\n",
    "    Argumenty:\n",
    "        sent: Zdanie do sparsowania.\n",
    "        distance_model: Wytrenowany model odległości\n",
    "        depth_model: Wytrenowany model głębokości\n",
    "        tokenizer: Tokenizator HERBERT\n",
    "        model: Model HERBERT\n",
    "\n",
    "    Zwraca:\n",
    "        ParsedSentence: Zdanie z przewidzianym drzewem składniowym.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Twoje rozwiązanie powinno:\n",
    "    # 1. Uzyskać embeddingi słów dla zdania.\n",
    "    # 2. Wybrać korzeń drzewa składniowego heurystycznie, używając depth_model.\n",
    "    # 3. Obliczyć odległości między każdą parą węzłów, używając distance_model.\n",
    "    # 4. Zaimplementować wymyśloną przez Ciebie heurystyczną metodę wyboru krawędzi \n",
    "    #    drzewa na podstawie przewidywanych odległości.\n",
    "    # 5. Uzyskać obiekt ParsedSentence. Możesz użyć funkcji ParsedSentence.from_edges_and_root\n",
    "    # 6. Zwróć obiekt ParsedSentence.\n",
    "\n",
    "    # Wskazówki:\n",
    "    #  Możesz użyć sent.pretty_print() do wizualizacji sparsowanego zdania.\n",
    "\n",
    "    # Uwaga:\n",
    "    # Ta funkcja zostanie użyta do oceny twojego rozwiązania. Ta funkcja powinna zwrócić rzeczywiste drzewo,\n",
    "    # z len(sent) - 1 krawędziami. Jeśli twoje przewidywanie nie będzie drzewem, będzie ono nieprawidłowe i\n",
    "    # nie zdobędziesz za nie punktów. Jeśli chcesz zdobyć tylko część punktów, konkurując tylko w metryce \n",
    "    # root placement, nadal powinieneś zwrócić poprawne drzewo.\n",
    "\n",
    "    in_len = len(sent)\n",
    "\n",
    "    embeddings = get_word_embeddings([sent], tokenizer, model)[0]\n",
    "    embeddings = torch.unsqueeze(embeddings, 0)\n",
    "    input_lengths = torch.tensor([in_len])\n",
    "\n",
    "    depth_model.eval()\n",
    "    depth_outputs = depth_model(embeddings, input_lengths)[0].detach().numpy()\n",
    "\n",
    "    distance_model.eval()\n",
    "    distance_outputs = distance_model(embeddings, input_lengths)[0].detach().numpy()\n",
    "\n",
    "    root = np.argmin(depth_outputs)\n",
    "\n",
    "    s_distances = []\n",
    "    for x in range(in_len):\n",
    "        for y in range(x + 1, in_len):\n",
    "            s_distances.append(((distance_outputs[x][y] + distance_outputs[y][x]) / 2, x, y))\n",
    "    s_distances.sort()\n",
    "\n",
    "\n",
    "    uf = UnionFind(in_len)\n",
    "    edges = []\n",
    "    for c, x, y in s_distances:\n",
    "        if not uf.is_connected(x, y):\n",
    "            edges.append((x, y))\n",
    "            uf.union(x, y)\n",
    "        if len(edges) >= in_len - 1:\n",
    "            break\n",
    "\n",
    "    my_sent = ParsedSentence.from_edges_and_root(sent.words, edges, root)\n",
    "    \n",
    "    return my_sent\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    sent = train_sentences[30]\n",
    "    parse_sentence(sent, distance_model, depth_model, tokenizer, model).pretty_print()  # Przewidziane drzewo\n",
    "    sent.pretty_print()  # Złote drzewo (ze zbioru danych)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "Kod bardzo podobny do poniższego będzie służył do ewaluacji rozwiązania na zdaniach testowych. Wywołując poniższe komórki możesz dowiedzieć się ile punktów zdobyłoby twoje rozwiązanie, gdybyśmy ocenili je na danych walidacyjnych. Przed wysłaniem rozwiązania upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "    return (scale(root_placement) + scale(uuas))\n",
    "\n",
    "def evaluate_model(sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model):\n",
    "    sum_uuas = 0\n",
    "    root_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sent in sentences:\n",
    "            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n",
    "            root_correct += int(parsed.root == sent.root)\n",
    "            sum_uuas += uuas_score(sent, parsed)\n",
    "    \n",
    "    root_placement = root_correct / len(sentences)\n",
    "    uuas = sum_uuas / len(sentences)\n",
    "\n",
    "    print(f\"UUAS: {uuas * 100:.3}%\")\n",
    "    print(f\"Root placement: {root_placement * 100:.3}%\")\n",
    "    print(f\"Your score: {points(root_placement, uuas):.3}/2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    distance_model_loaded = DistanceModel()\n",
    "    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n",
    "\n",
    "    depth_model_loaded = DepthModel()\n",
    "    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n",
    "\n",
    "    evaluate_model(val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
